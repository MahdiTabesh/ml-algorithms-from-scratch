{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9947d362-a0cb-42a5-abca-6403268c2978",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Learning Rate Schedules\n",
    "\n",
    "During training, the **learning rate (LR)** controls how large each parameter update is.  \n",
    "Instead of keeping LR constant, we can change it over time to improve convergence.  \n",
    "This process is called a **learning rate schedule**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Step Decay\n",
    "\n",
    "**Concept:**  \n",
    "- Keep LR constant for a while, then reduce it suddenly (in \"steps\").  \n",
    "- Often used when the loss plateaus after certain epochs.  \n",
    "- Produces abrupt drops in LR.\n",
    "\n",
    "**Formula:**  \n",
    "\n",
    "$$\n",
    "\\alpha_t = \\alpha_0 \\cdot \\gamma^{\\left\\lfloor \\frac{t}{T_{\\text{step}}} \\right\\rfloor}\n",
    "$$\n",
    "\n",
    "- $ \\alpha_0 $ : initial learning rate  \n",
    "- $ \\gamma \\in (0,1) $ : decay factor (e.g., 0.1)  \n",
    "- $ T_{\\text{step}} $ : interval of steps/epochs between drops  \n",
    "- $ t $ : current step or epoch  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Cosine Annealing\n",
    "\n",
    "**Concept:**  \n",
    "- Smoothly decrease LR following a half-cosine curve.  \n",
    "- Starts at a maximum ($ \\alpha_0 $) and ends at a minimum ($ \\eta_{\\min} $).  \n",
    "- Avoids sudden jumps, often yields better minima.\n",
    "\n",
    "**Formula:**  \n",
    "\n",
    "$$\n",
    "\\alpha_t = \\eta_{\\min} + \\tfrac{1}{2}(\\alpha_0 - \\eta_{\\min}) \n",
    "\\left(1 + \\cos\\!\\left(\\frac{\\pi t}{T_{\\max}}\\right)\\right)\n",
    "$$\n",
    "\n",
    "- $ \\alpha_0 $ : initial learning rate  \n",
    "- $ \\eta_{\\min} $ : final (minimum) learning rate  \n",
    "- $ T_{\\max} $ : total number of steps/epochs in the schedule  \n",
    "- $ t $ : current step or epoch  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”‘ Notes\n",
    "- **Epoch vs Step:** schedules can be applied per-epoch or per-mini-batch step.  \n",
    "- **Step Decay:** simple and effective, but has abrupt changes.  \n",
    "- **Cosine Annealing:** smooth decay, often preferred in modern deep learning.  \n",
    "- Both can be combined with a **warm-up phase** (gradual LR increase at start).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d495b954-fafb-466a-9d3a-7e8eedcb72e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3d4e4b-9af5-44ca-8af1-84f555bc942d",
   "metadata": {},
   "source": [
    "## Step Decay â€” from scratch (short recap)\n",
    "\n",
    "- Drops LR by a constant factor every fixed interval.\n",
    "- **Formula (epoch/step \\(t\\)):**\n",
    "\n",
    "  $$\n",
    "  \\alpha_t \\;=\\; \\alpha_0 \\cdot \\gamma^{\\left\\lfloor \\frac{t}{T_{\\text{step}}} \\right\\rfloor}\n",
    "  $$\n",
    "\n",
    "- Use when loss plateaus at predictable times; simple but abrupt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a41c652-f4de-4a54-a506-184fdc399702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step Decay (compact)\n",
    "from math import floor\n",
    "\n",
    "class StepDecayLR:\n",
    "    \"\"\"alpha_t = alpha0 * gamma^(floor(t/step_size))\"\"\"\n",
    "    def __init__(self, optimizer, step_size: int, gamma: float = 0.1):\n",
    "        self.opt = optimizer\n",
    "        self.step_size = int(step_size)\n",
    "        self.gamma = float(gamma)\n",
    "        self.t = -1\n",
    "        self.base = [g[\"lr\"] for g in self.opt.param_groups]\n",
    "        self.step()  # set t=0\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        k = self.t // self.step_size\n",
    "        for g, b in zip(self.opt.param_groups, self.base):\n",
    "            g[\"lr\"] = b * (self.gamma ** k)\n",
    "\n",
    "    def last_lr(self):\n",
    "        return [g[\"lr\"] for g in self.opt.param_groups]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6132cab9-243a-44a7-b8e5-7fb6a53be7f4",
   "metadata": {},
   "source": [
    "## Cosine Annealing â€” from scratch (short recap)\n",
    "\n",
    "- **Formula:**\n",
    "\n",
    "  $$\n",
    "  \\alpha_t \\;=\\; \\eta_{\\min} \\;+\\; \\tfrac{1}{2}(\\alpha_0 - \\eta_{\\min})\n",
    "  \\left( 1 + \\cos\\!\\left( \\frac{\\pi t}{T_{\\max}} \\right) \\right)\n",
    "  $$\n",
    "\n",
    "- Avoids abrupt jumps; often improves late-stage convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acb80a69-ac94-4dc7-8797-59ddb9a027b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Annealing (compact, no restarts)\n",
    "from math import pi, cos\n",
    "\n",
    "class CosineAnnealingLR:\n",
    "    \"\"\"alpha_t = eta_min + 0.5*(alpha0-eta_min)*(1+cos(pi*t/T_max))\"\"\"\n",
    "    def __init__(self, optimizer, T_max: int, eta_min: float = 0.0):\n",
    "        self.opt = optimizer\n",
    "        self.T_max = int(T_max)\n",
    "        self.eta_min = float(eta_min)\n",
    "        self.t = -1\n",
    "        self.base = [g[\"lr\"] for g in self.opt.param_groups]\n",
    "        self.step()  # set t=0\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        tt = min(max(self.t, 0), self.T_max)\n",
    "        for g, b in zip(self.opt.param_groups, self.base):\n",
    "            g[\"lr\"] = self.eta_min + 0.5 * (b - self.eta_min) * (1 + cos(pi * tt / self.T_max))\n",
    "\n",
    "    def last_lr(self):\n",
    "        return [g[\"lr\"] for g in self.opt.param_groups]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5ac0c8-893e-4a4f-b390-5062681ebd04",
   "metadata": {},
   "source": [
    "## Quick experiment on toy data (uses the custom schedulers)\n",
    "\n",
    "- Trains a tiny MLP on random data for a few epochs.  \n",
    "- Switch the scheduler block to compare Step Decay vs. Cosine.  \n",
    "- Logs LR each epoch so you can see the schedule working.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c076b7cf-9eba-4814-affe-170585d415e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 | lr=0.099391 | loss=8.1559\n",
      "epoch 02 | lr=0.097577 | loss=6.6710\n",
      "epoch 03 | lr=0.094605 | loss=5.3163\n",
      "epoch 04 | lr=0.090546 | loss=3.8573\n",
      "epoch 05 | lr=0.085502 | loss=2.4531\n",
      "epoch 06 | lr=0.079595 | loss=1.3830\n",
      "epoch 07 | lr=0.072973 | loss=0.7649\n",
      "epoch 08 | lr=0.065796 | loss=0.4699\n",
      "epoch 09 | lr=0.058244 | loss=0.3321\n",
      "epoch 10 | lr=0.050500 | loss=0.2622\n",
      "epoch 11 | lr=0.042756 | loss=0.2230\n",
      "epoch 12 | lr=0.035204 | loss=0.1989\n",
      "epoch 13 | lr=0.028027 | loss=0.1832\n",
      "epoch 14 | lr=0.021405 | loss=0.1727\n",
      "epoch 15 | lr=0.015498 | loss=0.1655\n",
      "epoch 16 | lr=0.010454 | loss=0.1606\n",
      "epoch 17 | lr=0.006395 | loss=0.1574\n",
      "epoch 18 | lr=0.003423 | loss=0.1553\n",
      "epoch 19 | lr=0.001609 | loss=0.1540\n",
      "epoch 20 | lr=0.001000 | loss=0.1534\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(7)\n",
    "\n",
    "x = torch.randn(512, 10)\n",
    "true_w = torch.randn(10, 1)\n",
    "y = x @ true_w + 0.1 * torch.randn(512, 1)\n",
    "\n",
    "model = nn.Sequential(nn.Linear(10, 32), nn.ReLU(), nn.Linear(32, 1))\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# ---- choose ONE ----\n",
    "#sched = StepDecayLR(opt, step_size=5, gamma=0.1)\n",
    "sched = CosineAnnealingLR(opt, T_max=20, eta_min=1e-3)\n",
    "\n",
    "EPOCHS = 20\n",
    "for ep in range(EPOCHS):\n",
    "    opt.zero_grad()\n",
    "    loss = loss_fn(model(x), y)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    sched.step()\n",
    "    print(f\"epoch {ep+1:02d} | lr={sched.last_lr()[0]:.6f} | loss={loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303fbd24-6375-4c12-aa3e-998028b5ef89",
   "metadata": {},
   "source": [
    "## Using PyTorch built-ins for comparison\n",
    "\n",
    "- `torch.optim.lr_scheduler.StepLR`  \n",
    "- `torch.optim.lr_scheduler.CosineAnnealingLR`  \n",
    "(identical usage pattern: call `scheduler.step()` at your chosen cadence.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2b916d4-579c-404a-96f0-788ca2a3f887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[builtin] epoch 01 | lr=0.099391 | loss=8.1559\n",
      "[builtin] epoch 02 | lr=0.097577 | loss=6.6710\n",
      "[builtin] epoch 03 | lr=0.094605 | loss=5.3163\n",
      "[builtin] epoch 04 | lr=0.090546 | loss=3.8573\n",
      "[builtin] epoch 05 | lr=0.085502 | loss=2.4531\n",
      "[builtin] epoch 06 | lr=0.079595 | loss=1.3830\n",
      "[builtin] epoch 07 | lr=0.072973 | loss=0.7649\n",
      "[builtin] epoch 08 | lr=0.065796 | loss=0.4699\n",
      "[builtin] epoch 09 | lr=0.058244 | loss=0.3321\n",
      "[builtin] epoch 10 | lr=0.050500 | loss=0.2622\n",
      "[builtin] epoch 11 | lr=0.042756 | loss=0.2230\n",
      "[builtin] epoch 12 | lr=0.035204 | loss=0.1989\n",
      "[builtin] epoch 13 | lr=0.028027 | loss=0.1832\n",
      "[builtin] epoch 14 | lr=0.021405 | loss=0.1727\n",
      "[builtin] epoch 15 | lr=0.015498 | loss=0.1655\n",
      "[builtin] epoch 16 | lr=0.010454 | loss=0.1606\n",
      "[builtin] epoch 17 | lr=0.006395 | loss=0.1574\n",
      "[builtin] epoch 18 | lr=0.003423 | loss=0.1553\n",
      "[builtin] epoch 19 | lr=0.001609 | loss=0.1540\n",
      "[builtin] epoch 20 | lr=0.001000 | loss=0.1534\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(7)\n",
    "\n",
    "x = torch.randn(512, 10)\n",
    "y = x @ torch.randn(10, 1) + 0.1 * torch.randn(512, 1)\n",
    "\n",
    "m = nn.Sequential(nn.Linear(10, 32), nn.ReLU(), nn.Linear(32, 1))\n",
    "opt = torch.optim.SGD(m.parameters(), lr=0.1)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "#sch = torch.optim.lr_scheduler.StepLR(opt, step_size=5, gamma=0.1)\n",
    "sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=20, eta_min=1e-3)\n",
    "\n",
    "for ep in range(20):\n",
    "    opt.zero_grad()\n",
    "    loss = loss_fn(m(x), y)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    sch.step()\n",
    "    print(f\"[builtin] epoch {ep+1:02d} | lr={opt.param_groups[0]['lr']:.6f} | loss={loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3552a6-cb6d-48ef-8300-9bd4f2a29807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-dev)",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
