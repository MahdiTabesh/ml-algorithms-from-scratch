{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "557b97d7-7120-49d5-9aed-bdaa0b5b8873",
   "metadata": {},
   "source": [
    "# Gradient Clipping & Gradient Checking\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Gradient Clipping\n",
    "**Definition:**  \n",
    "A technique to prevent the **exploding gradient problem** by restricting the magnitude of gradients before applying parameter updates.  \n",
    "\n",
    "**Why use it?**  \n",
    "- Stabilizes training (especially in deep networks & RNNs).  \n",
    "- Prevents weights from taking very large steps.  \n",
    "\n",
    "**Types:**\n",
    "1. **Norm Clipping (preferred):** Scale all gradients down proportionally if their overall norm exceeds a threshold.  \n",
    "\n",
    "$$\n",
    "g \\; \\leftarrow \\; \\frac{g}{\\max\\left(1, \\frac{\\|g\\|}{\\tau}\\right)}\n",
    "$$  \n",
    "\n",
    "where:  \n",
    "- $g$ = gradient vector  \n",
    "- $\\tau$ = threshold  \n",
    "\n",
    " \n",
    "\n",
    "2. **Value Clipping:** Clamp each gradient component to a fixed range:  \n",
    "\n",
    "$$\n",
    "g_i \\; \\in [-c, c]\n",
    "$$  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Gradient Checking\n",
    "**Definition:**  \n",
    "A debugging technique to verify correctness of **backpropagation** by comparing analytical gradients with numerical approximations.  \n",
    "\n",
    "**Why use it?**  \n",
    "- Detects errors in manual derivative implementation.  \n",
    "- Ensures your backprop matches expected values.  \n",
    "\n",
    "**Formula (Central Difference Approximation):**  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\theta_i} \\;\\approx\\; \n",
    "\\frac{J(\\theta_i + \\epsilon) - J(\\theta_i - \\epsilon)}{2\\epsilon}\n",
    "$$  \n",
    "\n",
    "where:  \n",
    "- $J$ = loss function  \n",
    "- $\\theta_i$ = parameter  \n",
    "- $\\epsilon$ = small value (e.g., $10^{-5}$)  \n",
    "\n",
    "**Process:**  \n",
    "1. Choose a parameter $\\theta_i$.  \n",
    "2. Perturb by $+\\epsilon$ and $-\\epsilon$.  \n",
    "3. Compute the loss in both cases.  \n",
    "4. Compare with the analytical gradient.  \n",
    "\n",
    "âœ… If the relative error is small (e.g., $< 10^{-7}$), your gradients are correct.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Summary\n",
    "- **Gradient Clipping** â†’ Used *during training* to stabilize updates.  \n",
    "- **Gradient Checking** â†’ Used *during debugging* to validate backprop.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8fc8b-c60d-493d-b0b5-15c89823fecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ecdd17f-a3a3-4544-818e-fbbb48bb9722",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm_, clip_grad_value_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc099919-650a-4460-9617-cecde67e8db3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "853708cf-dc61-45cc-be50-bdabf70bb78c",
   "metadata": {},
   "source": [
    "## Gradient Clipping (NumPy demo)\n",
    "\n",
    "Weâ€™ll implement two clipping methods:\n",
    "\n",
    "- **Norm clipping**: scale the whole gradient so its L2 norm â‰¤ Ï„  \n",
    "\n",
    "$$\n",
    "g \\;\\leftarrow\\; \\frac{g}{\\max\\!\\Big(1,\\; \\frac{\\|g\\|_2}{\\tau}\\Big)}\n",
    "$$  \n",
    "\n",
    "- **Value clipping**: clamp each component to a fixed range  \n",
    "\n",
    "$$\n",
    "g_i \\;\\in\\; [-c, c]\n",
    "$$  \n",
    "\n",
    "Weâ€™ll print the gradient **before/after** to see the effect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49997d73-655f-4104-934c-6ca10f9bc98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy demo: gradient clipping (norm & value)\n",
    "import numpy as np\n",
    "\n",
    "def clip_by_norm(g, tau: float):\n",
    "    \"\"\"\n",
    "    Scale gradient vector g so that ||g||_2 <= tau (if needed).\n",
    "    \"\"\"\n",
    "    norm = np.linalg.norm(g, ord=2)\n",
    "    scale = max(1.0, norm / tau)\n",
    "    return g / scale, norm, np.linalg.norm(g / scale, ord=2)\n",
    "\n",
    "def clip_by_value(g, c: float):\n",
    "    \"\"\"\n",
    "    Clamp each component of g to [-c, c].\n",
    "    \"\"\"\n",
    "    g_clipped = np.clip(g, -c, c)\n",
    "    return g_clipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6a16bc-b91d-4ad3-a32f-0f73898246d4",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f45b959b-0bfb-43e2-81b8-2dee4a910a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original g: [ 0.629 -0.661  3.202  0.525 -2.678  1.808  6.52   4.735]\n",
      "Original ||g||: 9.313\n",
      "\n",
      "[Norm clipping]\n",
      "Ï„ = 5.0\n",
      "||g|| before: 9.313\n",
      "||g|| after : 5.0\n",
      "Clipped g   : [ 0.338 -0.355  1.719  0.282 -1.438  0.971  3.5    2.542]\n",
      "\n",
      "[Value clipping]\n",
      "c = 2.0\n",
      "Clipped g   : [ 0.629 -0.661  2.     0.525 -2.     1.808  2.     2.   ]\n",
      "||g|| after : 4.514\n"
     ]
    }
   ],
   "source": [
    "# Example usage of our clipping functions\n",
    "rng = np.random.default_rng(0)\n",
    "g = rng.normal(0.0, 5.0, size=8)   # pretend gradient vector\n",
    "\n",
    "print(\"Original g:\", np.round(g, 3))\n",
    "print(\"Original ||g||:\", round(np.linalg.norm(g), 3))\n",
    "\n",
    "# ---- Norm clipping ----\n",
    "tau = 5.0\n",
    "g_norm, norm_before, norm_after = clip_by_norm(g, tau)\n",
    "print(\"\\n[Norm clipping]\")\n",
    "print(\"Ï„ =\", tau)\n",
    "print(\"||g|| before:\", round(norm_before, 3))\n",
    "print(\"||g|| after :\", round(norm_after, 3))\n",
    "print(\"Clipped g   :\", np.round(g_norm, 3))\n",
    "\n",
    "# ---- Value clipping ----\n",
    "c = 2.0\n",
    "g_val = clip_by_value(g, c)\n",
    "print(\"\\n[Value clipping]\")\n",
    "print(\"c =\", c)\n",
    "print(\"Clipped g   :\", np.round(g_val, 3))\n",
    "print(\"||g|| after :\", round(np.linalg.norm(g_val), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d25830-dad8-488f-ba8c-3edefc3c42cf",
   "metadata": {},
   "source": [
    "## Gradient Checking (NumPy, finite differences)\n",
    "\n",
    "We check our analytic gradient against a numerical approximation using:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\theta_i} \\;\\approx\\; \n",
    "\\frac{J(\\theta_i + \\epsilon) - J(\\theta_i - \\epsilon)}{2\\epsilon}\n",
    "$$  \n",
    "\n",
    "- Test function:  \n",
    "\n",
    "$$\n",
    "J(\\theta) = \\tfrac{1}{2}\\,\\theta^\\top A \\theta + b^\\top \\theta\n",
    "$$  \n",
    "\n",
    "Analytic gradient:  \n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\tfrac{1}{2}(A + A^\\top)\\theta + b\n",
    "$$  \n",
    "\n",
    "Relative error metric:  \n",
    "\n",
    "$$\n",
    "\\text{rel\\_err} \\;=\\; \\frac{\\|\\nabla J_{\\text{analytic}} - \\nabla J_{\\text{numeric}}\\|_2}\n",
    "{\\|\\nabla J_{\\text{analytic}}\\|_2 + \\|\\nabla J_{\\text{numeric}}\\|_2 + 10^{-12}}\n",
    "$$  \n",
    "\n",
    "A value below ~\\(10^{-7}\\) means the gradients match very well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "889fed45-8359-48b8-8201-2e1c4bd917d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy demo: gradient checking (finite differences)\n",
    "import numpy as np\n",
    "\n",
    "def loss(theta, A, b):\n",
    "    \"\"\"Quadratic loss: J(Î¸) = 0.5 Î¸áµ€ A Î¸ + báµ€ Î¸\"\"\"\n",
    "    return 0.5 * theta @ (A @ theta) + b @ theta\n",
    "\n",
    "def grad_analytic(theta, A, b):\n",
    "    \"\"\"Analytic gradient: âˆ‡J(Î¸) = 0.5(A + Aáµ€)Î¸ + b\"\"\"\n",
    "    return 0.5 * (A + A.T) @ theta + b\n",
    "\n",
    "def grad_numeric(theta, A, b, eps=1e-5):\n",
    "    \"\"\"Numeric gradient via central difference.\"\"\"\n",
    "    g = np.zeros_like(theta)\n",
    "    for i in range(len(theta)):\n",
    "        e = np.zeros_like(theta); e[i] = 1.0\n",
    "        g[i] = (loss(theta + eps*e, A, b) - loss(theta - eps*e, A, b)) / (2*eps)\n",
    "    return g\n",
    "\n",
    "def relative_error(g1, g2):\n",
    "    \"\"\"Compare two gradients with relative error.\"\"\"\n",
    "    num = np.linalg.norm(g1 - g2)\n",
    "    den = np.linalg.norm(g1) + np.linalg.norm(g2) + 1e-12\n",
    "    return num / den\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f651f767-b70f-4f10-94de-4ce1e163a4e2",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67e1ae78-4ace-4894-90c5-310cd8b77202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î¸: [ 0.155  0.378 -0.888 -1.981 -0.348]\n",
      "\n",
      "Analytic grad: [-10.905588  -1.915013  -6.432957 -10.842264  -9.894571]\n",
      "Numeric grad : [-10.905588  -1.915013  -6.432957 -10.842264  -9.894571]\n",
      "\n",
      "Relative error: 3.153475444993319e-12\n",
      "Gradients match!\n"
     ]
    }
   ],
   "source": [
    "# Example usage of gradient checking\n",
    "np.random.seed(0)\n",
    "d = 5\n",
    "\n",
    "# random quadratic problem\n",
    "A = np.random.randn(d, d)\n",
    "A = A.T @ A + 0.1*np.eye(d)\n",
    "b = np.random.randn(d)\n",
    "theta = np.random.randn(d)\n",
    "\n",
    "# compute gradients\n",
    "g_ana = grad_analytic(theta, A, b)\n",
    "g_num = grad_numeric(theta, A, b, eps=1e-5)\n",
    "err = relative_error(g_ana, g_num)\n",
    "\n",
    "# print results\n",
    "print(\"Î¸:\", np.round(theta, 3))\n",
    "print(\"\\nAnalytic grad:\", np.round(g_ana, 6))\n",
    "print(\"Numeric grad :\", np.round(g_num, 6))\n",
    "print(\"\\nRelative error:\", err)\n",
    "if err < 1e-7:\n",
    "    print(\"Gradients match!\")\n",
    "else:\n",
    "    print(\"Gradients differ!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a646d-2b31-4676-b75e-08d63f298817",
   "metadata": {},
   "source": [
    "## Gradient Clipping in PyTorch (training loop)\n",
    "\n",
    "PyTorch optimizers **donâ€™t clip gradients automatically**.  \n",
    "You call a utility **before** `optimizer.step()`:\n",
    "\n",
    "```python\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=Ï„)\n",
    "# or\n",
    "torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4c83d0c-39cb-4212-8066-c16c5479e08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | Loss 5.6410 | Grad norm before 4.358 â†’ after 1.000\n",
      "Step 1 | Loss 4.8046 | Grad norm before 4.006 â†’ after 1.000\n",
      "Step 2 | Loss 4.0385 | Grad norm before 3.656 â†’ after 1.000\n",
      "Step 3 | Loss 3.3419 | Grad norm before 3.310 â†’ after 1.000\n",
      "Step 4 | Loss 2.7144 | Grad norm before 2.966 â†’ after 1.000\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils import clip_grad_norm_, clip_grad_value_\n",
    "\n",
    "# Example training loop\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# fake regression dataset\n",
    "X = torch.randn(64, 5)\n",
    "true_w = torch.randn(5, 1)\n",
    "y = X @ true_w + 0.1 * torch.randn(64, 1)\n",
    "\n",
    "# simple linear model\n",
    "model = nn.Linear(5, 1)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.2)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# choose thresholds\n",
    "tau = 1.0  # for norm clipping\n",
    "c = 0.5    # for value clipping\n",
    "\n",
    "for step in range(5):\n",
    "    opt.zero_grad()\n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "    loss.backward()\n",
    "\n",
    "    # --- apply clipping ---\n",
    "    grad_norm_before = torch.sqrt(sum((p.grad.norm()**2 for p in model.parameters())))\n",
    "    clip_grad_norm_(model.parameters(), max_norm=tau)   # norm clipping\n",
    "    \n",
    "    grad_norm_after = torch.sqrt(sum((p.grad.norm()**2 for p in model.parameters())))\n",
    "\n",
    "    opt.step()\n",
    "\n",
    "    print(f\"Step {step} | Loss {loss.item():.4f} | \"\n",
    "          f\"Grad norm before {grad_norm_before:.3f} â†’ after {grad_norm_after:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4fb5c2-09e2-4fa1-a200-a55292c31656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-dev)",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
