{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d154801-3862-4afe-9c07-47c701b1c42c",
   "metadata": {},
   "source": [
    "## Adam Optimizer\n",
    "\n",
    "Adam (**Adaptive Moment Estimation**) is an optimization algorithm that combines the ideas of:\n",
    "\n",
    "- **Momentum:** keeps an exponential moving average of past gradients (smooth direction).\n",
    "- **RMSProp:** keeps an exponential moving average of squared gradients (adaptive step size).\n",
    "- **Bias correction:** fixes the underestimation in early steps.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Formulas\n",
    "\n",
    "At step \\(t\\), given gradient:\n",
    "\n",
    "$$\n",
    "g_t = \\nabla_\\theta L(\\theta_t)\n",
    "$$\n",
    "\n",
    "\n",
    "1. **Update moving averages**\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n",
    "$$\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n",
    "$$\n",
    "\n",
    "2. **Bias correction**\n",
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad\n",
    "\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "\n",
    "3. **Parameter update**\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Hyperparameters (common defaults)\n",
    "\n",
    "- Learning rate:  \n",
    "$$ \\alpha = 0.001 $$  \n",
    "\n",
    "- Momentum decay:  \n",
    "$$ \\beta_1 = 0.9 $$  \n",
    "\n",
    "- Squared gradient decay:  \n",
    "$$ \\beta_2 = 0.999 $$  \n",
    "\n",
    "- Numerical stability:  \n",
    "$$ \\epsilon = 10^{-8} $$  \n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Intuition\n",
    "\n",
    "- **Momentum** \\((m_t)\\): remembers the direction, reduces oscillations.  \n",
    "- **RMSProp** \\((v_t)\\): adapts learning rate for each parameter.  \n",
    "- **Bias correction**: ensures stable updates in early iterations.  \n",
    "- **Together ‚Üí stable, adaptive, and efficient convergence.**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Pros\n",
    "‚úÖ Works well out-of-the-box  \n",
    "‚úÖ Fast convergence compared to SGD  \n",
    "‚úÖ Handles sparse/noisy gradients well  \n",
    "‚úÖ Adaptive learning rates per parameter  \n",
    "\n",
    "### üîπ Cons\n",
    "‚ùå More memory usage (stores \\(m\\) and \\(v\\) for each parameter)  \n",
    "‚ùå Can generalize worse than SGD in some cases  \n",
    "‚ùå Still needs learning rate scheduling sometimes  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "496f68d9-e794-4898-ba60-a75ae2a2f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6df062de-26cb-4c2b-a0e1-225f602a7944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam (from scratch)\n",
    "class AdamOptimizer:\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.beta1, self.beta2 = betas\n",
    "        self.eps = eps\n",
    "\n",
    "        # Per-parameter state\n",
    "        self.m = [torch.zeros_like(p) for p in self.params]  # first moment\n",
    "        self.v = [torch.zeros_like(p) for p in self.params]  # second moment\n",
    "        self.t = 0  # step counter\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        b1t = 1.0 - self.beta1 ** self.t   # for bias correction\n",
    "        b2t = 1.0 - self.beta2 ** self.t\n",
    "\n",
    "        for p, m, v in zip(self.params, self.m, self.v):\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "\n",
    "            g = p.grad\n",
    "\n",
    "            # 1) Update moving averages\n",
    "            m.mul_(self.beta1).add_(g, alpha=1.0 - self.beta1)          # m_t\n",
    "            v.mul_(self.beta2).addcmul_(g, g, value=1.0 - self.beta2)   # v_t\n",
    "\n",
    "            # 2) Bias-corrected moments\n",
    "            m_hat = m / b1t\n",
    "            v_hat = v / b2t\n",
    "\n",
    "            # 3) Parameter update\n",
    "            p.addcdiv_(m_hat, torch.sqrt(v_hat) + self.eps, value=-self.lr)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "333cc312-3211-4d7f-9939-a9fb2043edb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05: loss = 0.809774\n",
      "Epoch 10: loss = 0.721449\n",
      "Epoch 15: loss = 0.638959\n",
      "Epoch 20: loss = 0.562460\n",
      "Epoch 25: loss = 0.491979\n",
      "Epoch 30: loss = 0.428558\n",
      "Epoch 35: loss = 0.372196\n",
      "Epoch 40: loss = 0.320848\n",
      "Epoch 45: loss = 0.274360\n",
      "Epoch 50: loss = 0.232563\n"
     ]
    }
   ],
   "source": [
    "# Tiny sanity test: single-batch regression\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 16), torch.nn.ReLU(),\n",
    "    torch.nn.Linear(16, 1)\n",
    ")\n",
    "\n",
    "optimizer = AdamOptimizer(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-8)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Dummy data (single sample; goal is just to see loss go down)\n",
    "x = torch.tensor([[1.0, 2.0]])\n",
    "y = torch.tensor([[1.0]])\n",
    "\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:02d}: loss = {loss.item():.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-dev)",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
