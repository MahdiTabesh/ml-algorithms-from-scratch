{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "811147e6-6f6a-4bdb-be14-09d52e73fe0c",
   "metadata": {},
   "source": [
    "# RMSProp Optimizer (From Scratch)\n",
    "\n",
    "In this notebook, we implement **RMSProp** from scratch in PyTorch style and test it on a simple model.  \n",
    "RMSProp is an **adaptive learning rate optimizer** introduced by *Geoff Hinton* to stabilize training.  \n",
    "\n",
    "Unlike vanilla SGD, which uses the same learning rate for all parameters, RMSProp:  \n",
    "- Keeps a **running average of squared gradients**.  \n",
    "- Uses this average to **rescale step sizes** for each parameter.  \n",
    "- Prevents overshooting when gradients are large, and speeds up learning when gradients are small.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e990c49-6399-4b77-903e-64a13198149c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cf07035-57bc-462e-b47b-df0150313977",
   "metadata": {},
   "source": [
    "## ðŸ”¹ RMSProp Update Rule\n",
    "\n",
    "1. **Running average of squared gradients:**\n",
    "$$\n",
    "E[g^2]_t = \\gamma E[g^2]_{t-1} + (1 - \\gamma) g_t^2\n",
    "$$\n",
    "\n",
    "2. **Parameter update:**\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} g_t\n",
    "$$\n",
    "\n",
    "- $g_t$: gradient at step $t$  \n",
    "- $\\eta$: learning rate  \n",
    "- $\\gamma$: decay factor (e.g. 0.9)  \n",
    "- $\\epsilon$: small constant for stability  \n",
    "\n",
    "ðŸ‘‰ Intuition:  \n",
    "- Large recent gradients â†’ denominator grows â†’ smaller steps.  \n",
    "- Small recent gradients â†’ denominator shrinks â†’ larger steps.  \n",
    "- Each parameter adapts its learning rate individually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7403ac9f-ee2a-422b-83fb-eb278de30502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63ae2480-1d72-4253-806a-47027739da18",
   "metadata": {},
   "source": [
    "## RMSProp Implementation\n",
    "\n",
    "Below is a **from-scratch PyTorch-style implementation** of RMSProp.  \n",
    "We maintain a moving average of squared gradients for each parameter and use it to scale updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23c95703-42af-44ec-af08-b993a164d827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class RMSPropOptimizer:\n",
    "    def __init__(self, params, lr=0.01, gamma=0.9, eps=1e-8):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma                # decay rate for moving avg\n",
    "        self.eps = eps                    # numerical stability term\n",
    "        self.avg_sq_grad = [torch.zeros_like(p) for p in self.params]\n",
    "\n",
    "    def step(self):\n",
    "        # Update each parameter\n",
    "        for p, avg_sq in zip(self.params, self.avg_sq_grad):\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "\n",
    "            g = p.grad\n",
    "\n",
    "            # Update moving average of squared gradients\n",
    "            avg_sq.mul_(self.gamma).addcmul_(g, g, value=1 - self.gamma)\n",
    "\n",
    "            # Compute parameter update\n",
    "            p.data.addcdiv_(g, torch.sqrt(avg_sq + self.eps), value=-self.lr)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c11993-6ff1-4869-aabf-843d22c144f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "def356b7-51f8-4a11-a103-1dd75536044e",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Testing RMSProp on a Simple Linear Model\n",
    "\n",
    "We apply our RMSProp optimizer to a dummy regression task.  \n",
    "This demonstrates how the optimizer stabilizes learning compared to vanilla SGD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9c787bc-2380-4174-959d-8ac99b307a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss = 2.9726\n",
      "Epoch 2, Loss = 2.5525\n",
      "Epoch 3, Loss = 2.2779\n",
      "Epoch 4, Loss = 2.0650\n",
      "Epoch 5, Loss = 1.8882\n",
      "Epoch 6, Loss = 1.7356\n",
      "Epoch 7, Loss = 1.6008\n",
      "Epoch 8, Loss = 1.4799\n",
      "Epoch 9, Loss = 1.3700\n",
      "Epoch 10, Loss = 1.2694\n"
     ]
    }
   ],
   "source": [
    "# Dummy model\n",
    "model = torch.nn.Linear(2, 1)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = RMSPropOptimizer(model.parameters(), lr=0.01, gamma=0.9)\n",
    "\n",
    "# Dummy data\n",
    "x = torch.tensor([[1.0, 2.0]])\n",
    "y = torch.tensor([[1.0]])\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abca463f-2e39-4922-83ed-995118dce066",
   "metadata": {},
   "source": [
    "## âœ… Key Takeaways\n",
    "\n",
    "- **RMSProp â‰  plain SGD**: it adapts learning rates per parameter.  \n",
    "- Tracks the **running average of squared gradients** to normalize updates.  \n",
    "- Prevents instability from noisy or unbalanced gradients.  \n",
    "- Forms the foundation of **Adam**, which combines RMSProp with momentum.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c226185-a7ad-4208-93f0-ce467f0fd61b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-dev)",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
