{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d948171-2970-4e27-b416-1ed9f8d58360",
   "metadata": {},
   "source": [
    "# Vanilla Gradient Descent (From Scratch)\n",
    "\n",
    "In this notebook, we implement **logistic regression with a single feature** using **vanilla gradient descent**.  \n",
    "We will not use PyTorchâ€™s automatic differentiation (`requires_grad=True`); instead, we will:\n",
    "\n",
    "- Define a tiny dataset with one input feature and binary labels.\n",
    "- Initialize weights (`w`) and bias (`b`) manually.\n",
    "- Perform the **forward pass** (linear function + sigmoid).\n",
    "- Compute the **binary cross-entropy loss** for monitoring.\n",
    "- Derive and apply **manual gradients** for both weight and bias.\n",
    "- Update parameters with **gradient descent**.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Goals\n",
    "- Understand the mechanics of gradient descent updates without relying on autograd.\n",
    "- Practice deriving gradients for logistic regression by hand.\n",
    "- Build intuition for how `w` and `b` evolve step by step to reduce loss.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Formulas\n",
    "- **Prediction (logits â†’ sigmoid):**\n",
    "\n",
    "$$\n",
    "z = xw + b, \\quad \\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "- **Binary Cross-Entropy Loss:**\n",
    "\n",
    "$$\n",
    "L = - \\frac{1}{N} \\sum \\Big( y \\log(\\hat{y}) + (1-y)\\log(1-\\hat{y}) \\Big)\n",
    "$$\n",
    "\n",
    "- **Gradients:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{1}{N} \\sum ( \\hat{y} - y )x, \n",
    "\\quad \n",
    "\\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum ( \\hat{y} - y )\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "By the end of this notebook, you will see **vanilla gradient descent in action** and understand how parameter updates reduce the loss step by step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc9c43-3700-4d3e-9488-d1460b11dddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3732b402-6b35-40ec-ab7c-1643db54c0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 0 is : 0.5901\n",
      "Loss in epoch 10 is : 0.5422\n",
      "Loss in epoch 20 is : 0.5035\n",
      "Loss in epoch 30 is : 0.4703\n",
      "Loss in epoch 40 is : 0.4414\n",
      "Loss in epoch 50 is : 0.4159\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "torch.manual_seed(42)\n",
    "\n",
    "#Dataset\n",
    "x = torch.tensor([0, 1, 2, 3])\n",
    "y = torch.tensor([0, 0, 1, 1])\n",
    "\n",
    "#Parameters\n",
    "w = torch.randn(1)\n",
    "b = torch.randn(1)\n",
    "\n",
    "#Hyperparameters\n",
    "epochs = 50\n",
    "lr = 0.1\n",
    "\n",
    "#training loop\n",
    "for epoch in range(epochs+1):\n",
    "    z = x * w + b\n",
    "    y_hat = torch.sigmoid(z)\n",
    "\n",
    "    loss = -(y*torch.log(y_hat + 1e-8) + (1-y) * torch.log(1-y_hat + 1e-8)).mean()\n",
    "\n",
    "    dz = y_hat - y\n",
    "    dw = (x * dz).mean()\n",
    "    db = dz.mean()\n",
    "\n",
    "    #gradient Update \n",
    "    w -= lr * dw\n",
    "    b -= lr * db\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Loss in epoch {epoch} is : {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a3342f-9eae-47ce-b9a4-8752fb21727a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7ed02f1-b28d-44a5-9c96-de2f83b7af86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: w = 2.1888, b = -0.6667, loss = 0.5521\n",
      "Step 11: w = 2.0109, b = -0.9328, loss = 0.4471\n",
      "Step 21: w = 1.8670, b = -1.1601, loss = 0.3724\n",
      "Step 31: w = 1.7611, b = -1.3504, loss = 0.3231\n",
      "Step 41: w = 1.6927, b = -1.5080, loss = 0.2923\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Training loop\n",
    "for step in range(50):\n",
    "    # Forward pass\n",
    "    z = x * w + b\n",
    "    y_hat = torch.sigmoid(z)\n",
    "    \n",
    "    # Binary cross-entropy loss\n",
    "    loss = -(y * torch.log(y_hat + 1e-8) + (1 - y) * torch.log(1 - y_hat + 1e-8)).mean()\n",
    "    \n",
    "    # Backward pass: compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters (vanilla GD)\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    \n",
    "    # Zero gradients for next step\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step+1}: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe4c80-732c-4bbc-94d0-4cb6ccdb0b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df816f39-b12a-4492-aeb1-d1120d282483",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- Implemented logistic regression with **vanilla gradient descent**.\n",
    "- Derived and applied **manual gradients** for weight and bias.\n",
    "- Verified results using **PyTorch autograd**.\n",
    "- Observed how parameters `w` and `b` evolve while the loss decreases.\n",
    "\n",
    "ðŸ“Œ This notebook builds intuition for how gradient descent works under the hood before moving to larger models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd67fd4-08ed-41c5-9082-0b5d960cd7d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-dev)",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
