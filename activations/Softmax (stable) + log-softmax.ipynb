{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be18d195-a8b3-4475-b648-cf1432897a46",
   "metadata": {},
   "source": [
    "# Softmax, Stable Softmax and Log-Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a39de0-41a3-48c7-b1ea-24d4a991e9a3",
   "metadata": {},
   "source": [
    "### What & Why (brief)\n",
    "\n",
    "Softmax turns a vector of scores **z** into probabilities:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Numerical issue:**  \n",
    "\\( e^z \\) explodes for large \\( z \\).\n",
    "\n",
    "**Fix:** subtract the max (doesnâ€™t change the probabilities):\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i - \\max(z)}}{\\sum_j e^{z_j - \\max(z)}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Log-Softmax:**  \n",
    "\n",
    "$$\n",
    "\\log \\text{softmax}(z) = z - \\log \\left(\\sum_j e^{z_j}\\right)\n",
    "$$\n",
    "\n",
    "(computed with *log-sum-exp* for stability).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6507171e-954e-4fda-af04-00a32b20ca50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bece9b6f-7635-41bb-869e-c64a371217ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def softmax_naive(x):\n",
    "    exp_x = np.exp(x)\n",
    "    print(x)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "def stable_softmax(x, axis= -1):\n",
    "    x_shifted = x - np.max(x, axis=axis, keepdims=True) #shift for Stability\n",
    "    exp_x = np.exp(x_shifted)\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "def log_softmax(x, axis= -1):\n",
    "    x_shifted = x - np.max(x, axis=axis, keepdims=True)\n",
    "    return x_shifted - np.log(np.sum(np.exp(x_shifted),axis=axis, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8565d540-b79c-4c46-ac75-104519b70838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3.]\n",
      "Softmax Naive: [0.09003057 0.24472847 0.66524096]\n",
      "\n",
      "Stable Softmax: [0.09003057 0.24472847 0.66524096]\n",
      "\n",
      "Log Softmax: [-2.40760596 -1.40760596 -0.40760596]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1.0, 2.0, 3.0])\n",
    "\n",
    "print(f'Softmax Naive: {softmax_naive(x)}\\n')\n",
    "print(f'Stable Softmax: {stable_softmax(x)}\\n')\n",
    "print(f'Log Softmax: {log_softmax(x)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb3625df-4d08-4d29-b9ee-c05c6d52bae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000. 1001. 1002.]\n",
      "Softmax naive: [nan nan nan]\n",
      "Softmax stable: [0.09003057 0.24472847 0.66524096]\n",
      "Log-Softmax: [-2.40760596 -1.40760596 -0.40760596]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_28766/325573457.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  exp_x = np.exp(x)\n",
      "/var/tmp/ipykernel_28766/325573457.py:6: RuntimeWarning: invalid value encountered in divide\n",
      "  return exp_x / np.sum(exp_x)\n"
     ]
    }
   ],
   "source": [
    "x_big = np.array([1000.0, 1001.0, 1002.0])\n",
    "\n",
    "print(\"Softmax naive:\", softmax_naive(x_big))   # will overflow -> NaN\n",
    "print(\"Softmax stable:\", stable_softmax(x_big)) # works fine\n",
    "print(\"Log-Softmax:\", log_softmax(x_big))       # works fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae0aeb-77f8-4e08-81ed-aa4c84e4f1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "654fe76c-0987-4bf5-a8c2-c7878719b1a0",
   "metadata": {},
   "source": [
    "# Pytorch Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0cbb3b1-656a-4ed4-ab84-e7fcaf0a962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def softmax_naive(x, dim=-1):\n",
    "    exp_x = torch.exp(x)\n",
    "    return exp_x / torch.sum(exp_x, dim=dim, keepdim=True)\n",
    "\n",
    "def softmax_stable(x, dim=-1):\n",
    "    x_shifted = x - torch.max(x, dim=dim, keepdim=True).values\n",
    "    exp_x = torch.exp(x_shifted)\n",
    "    return exp_x / torch.sum(exp_x, dim=dim, keepdim=True)\n",
    "\n",
    "def log_softmax(x, dim=-1):\n",
    "    x_shifted = x - torch.max(x, dim=dim, keepdim=True).values\n",
    "    log_sum_exp = torch.log(torch.sum(torch.exp(x_shifted), dim=dim, keepdim=True))\n",
    "    return x_shifted - log_sum_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c882ee1-15a7-4282-8ce5-f0d65f6b353e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Softmax:\n",
      " tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [   nan,    nan,    nan]])\n",
      "Stable Softmax:\n",
      " tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652]])\n",
      "Log-Softmax:\n",
      " tensor([[-2.4076, -1.4076, -0.4076],\n",
      "        [-2.4076, -1.4076, -0.4076]])\n",
      "torch.softmax:\n",
      " tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652]])\n",
      "torch.log_softmax:\n",
      " tensor([[-2.4076, -1.4076, -0.4076],\n",
      "        [-2.4076, -1.4076, -0.4076]])\n"
     ]
    }
   ],
   "source": [
    "# Example tensor\n",
    "t = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [1000.0, 1001.0, 1002.0]])\n",
    "\n",
    "print(\"Naive Softmax:\\n\", softmax_naive(t, dim=1))     # may overflow for large values\n",
    "print(\"Stable Softmax:\\n\", softmax_stable(t, dim=1))   # safe\n",
    "print(\"Log-Softmax:\\n\", log_softmax(t, dim=1))         # safe\n",
    "\n",
    "# Compare to PyTorch built-ins\n",
    "print(\"torch.softmax:\\n\", torch.softmax(t, dim=1))\n",
    "print(\"torch.log_softmax:\\n\", torch.log_softmax(t, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e879f-dd78-48cf-881e-72c14cd8f683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb82e462-aab7-4e7b-8341-41e54190083d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad61abd0-d1d7-4a32-86ed-907e96c6d971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a70e4f8-f9ed-439e-abfa-ae40002d1a36",
   "metadata": {},
   "source": [
    "## Using `CrossEntropyLoss` in PyTorch\n",
    "\n",
    "In neural networks, we usually output **raw logits** from the model and pass them directly into \n",
    "`torch.nn.CrossEntropyLoss`. \n",
    "\n",
    "This is because `CrossEntropyLoss` internally handles:\n",
    "1. Applying **log-softmax** (stable).\n",
    "2. Picking the log-probability of the correct class for each sample.\n",
    "3. Averaging (or summing) the negative log-likelihoods â†’ cross-entropy.\n",
    "\n",
    "ðŸ‘‰ So, we donâ€™t need to manually apply `torch.softmax` or `torch.log_softmax`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc663dea-d2cf-4c3a-8f45-75f97722b9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss: 1.4185397624969482\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Dummy logits (batch_size=2, num_classes=3)\n",
    "logits = torch.tensor([[2.0, 1.0, 0.1],\n",
    "                       [0.5, 2.5, 0.3]])\n",
    "\n",
    "# Dummy labels (true class indices)\n",
    "labels = torch.tensor([0, 2])\n",
    "\n",
    "# Define criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(logits, labels)\n",
    "print(\"Cross-Entropy Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60752467-50ae-4e96-96ea-0c431e7b3028",
   "metadata": {},
   "source": [
    "## What happens inside `CrossEntropyLoss`\n",
    "\n",
    "- **Step 1:** Applies `log_softmax(logits)` (with stability tricks).  \n",
    "- **Step 2:** Selects the log-probability of the correct class for each sample.  \n",
    "- **Step 3:** Averages (or sums) the negative log-probabilities.  \n",
    "\n",
    "---\n",
    "\n",
    "### The formula\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\frac{1}{N} \\sum_{i=1}^N \\log \n",
    "\\left( \\frac{e^{z_{i,y_i}}}{\\sum_j e^{z_{i,j}}} \\right)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "\n",
    "- \\( z_{i,j} \\) = logit for sample *i*, class *j*  \n",
    "- \\( y_i \\) = true label for sample *i*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6f8c67-8f2a-4a96-93d7-6c73c32b8f64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-dev)",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
