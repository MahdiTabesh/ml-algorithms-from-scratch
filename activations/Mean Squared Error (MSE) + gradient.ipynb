{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a9a0a53-c8fe-445f-87f8-19efc2016694",
   "metadata": {},
   "source": [
    "##  Mean Squared Error (MSE) Loss\n",
    "\n",
    "**Definition:**  \n",
    "The Mean Squared Error (MSE) measures the average squared difference between predictions and true values.  \n",
    "It is mainly used in **regression problems**, where the goal is to predict continuous values.\n",
    "\n",
    "---\n",
    "\n",
    "**Formula (per batch of size \\(n\\)):**\n",
    "\n",
    "$$\n",
    "L_{\\text{MSE}} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2\n",
    "$$\n",
    "\n",
    "- \\( \\hat{y}_i \\): model prediction  \n",
    "- \\( y_i \\): true target  \n",
    "- \\( n \\): number of samples  \n",
    "\n",
    "Sometimes a factor is included to simplify gradients:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2}, \\quad \\frac{1}{2n}\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Gradient with respect to predictions (\\(\\hat{y}_i\\)):**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_i} = \\frac{2}{n} (\\hat{y}_i - y_i)\n",
    "$$\n",
    "\n",
    "- If the \\( \\tfrac{1}{2n} \\) version is used, the gradient becomes:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_i} = \\frac{1}{n} (\\hat{y}_i - y_i)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Key Idea:**  \n",
    "- MSE penalizes large errors more strongly because of the squared term.  \n",
    "- Taking the mean ensures the loss is independent of batch size.  \n",
    "- The gradient points in the direction of reducing the gap between predictions and targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39108abf-dae8-4237-966b-b8ae026c5931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def mse_loss(y_hat, y, reduction= 'mean'):\n",
    "    diff = y_hat - y\n",
    "    loss = diff**2 \n",
    "\n",
    "    if reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    elif reduction == 'None':\n",
    "        return loss\n",
    "    else:\n",
    "        raise ValueError('reduction must be \"mean\" | \"sum\" | \"none\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08304133-3d99-40c7-95ee-d8edfa8042dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our mean loss: 2.255875825881958\n",
      "Torch Mean loss: 2.255875825881958\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "y = torch.randn(4, 3)\n",
    "y_hat = torch.randn(4, 3, requires_grad=True)\n",
    "\n",
    "print(\"Our mean loss:\", mse_loss(y_hat, y, reduction=\"mean\").item())\n",
    "\n",
    "import torch.nn as nn\n",
    "loss =nn.MSELoss()\n",
    "criterion = loss(y_hat, y)\n",
    "print(f'Torch Mean loss: {criterion}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd75b2a7-b8e4-4911-b75d-2ec13c57e74f",
   "metadata": {},
   "source": [
    "# Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8e1c02-1661-4030-91d4-29a5109a8d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_grad(y_hat, y, reduction='mean'):\n",
    "\n",
    "    diff = y_hat - y\n",
    "    if reduction == 'mean':\n",
    "        return (2 * diff)/y_hat.numel()\n",
    "    elif reduction == 'sum':\n",
    "        return 2 * diff\n",
    "    elif reduction == 'None':\n",
    "        return 2 * diff\n",
    "    else:\n",
    "        raise ValueError(\"reduction must be 'mean' | 'sum' | 'none'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05fbf657-bc54-418d-acf7-9acece591a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our loss: 2.255875825881958\n",
      "Torch loss: 2.255875825881958\n",
      "Autograd grad (first row): tensor([-0.3996,  0.2323,  0.1846])\n",
      "Manual grad (first row):  tensor([-0.3996,  0.2323,  0.1846])\n",
      "Max |diff|: 2.9802322387695312e-08\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "y = torch.randn(4, 3)\n",
    "y_hat = torch.randn(4, 3, requires_grad=True)\n",
    "\n",
    "# our loss\n",
    "L = mse_loss(y_hat, y, reduction=\"mean\")\n",
    "print(\"Our loss:\", L.item())\n",
    "\n",
    "# torch loss\n",
    "import torch.nn as nn\n",
    "torch_loss = nn.MSELoss(reduction=\"mean\")(y_hat, y)\n",
    "print(\"Torch loss:\", torch_loss.item())\n",
    "\n",
    "# gradients: autograd\n",
    "y_hat.grad = None\n",
    "L.backward()\n",
    "grad_autograd = y_hat.grad.detach().clone()\n",
    "\n",
    "# gradients: manual\n",
    "grad_manual = mse_grad(y_hat.detach(), y, reduction=\"mean\")\n",
    "\n",
    "print(\"Autograd grad (first row):\", grad_autograd[0])\n",
    "print(\"Manual grad (first row): \", grad_manual[0])\n",
    "print(\"Max |diff|:\", (grad_autograd - grad_manual).abs().max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ed94e-3595-4350-99b4-844398277d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-dev)",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
