{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcbd2c17-f07c-4f22-83bb-754d26b7fc42",
   "metadata": {},
   "source": [
    "# Cross-Entropy from Logits (Stable) + Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902e5b87-86f0-4c6d-bcdc-dba225698d40",
   "metadata": {},
   "source": [
    "## Multiclass Cross-Entropy (CE) Loss\n",
    "\n",
    "**1) Softmax (convert logits to probabilities)**  \n",
    "$$\n",
    "p_i = \\frac{e^{z_i}}{\\sum_{j=1}^{C} e^{z_j}}\n",
    "$$  \n",
    "Turns raw scores (*logits*) into probabilities that sum to 1.\n",
    "\n",
    "---\n",
    "\n",
    "**2) Cross-Entropy (one-hot labels)**  \n",
    "$$\n",
    "CE(p, y) = -\\sum_{i=1}^{C} y_i \\log p_i = -\\log p_{y^*}\n",
    "$$  \n",
    "Penalty is large if the true class \\(y^*\\) gets low probability.\n",
    "\n",
    "---\n",
    "\n",
    "**3) Stable CE directly from logits (log-sum-exp trick)**  \n",
    "$$\n",
    "CE(z, y^*) = \\log \\left( \\sum_{j=1}^{C} e^{z_j} \\right) - z_{y^*}\n",
    "$$  \n",
    "For numerical stability:  \n",
    "$$\n",
    "LSE(z) = m + \\log \\left( \\sum_{j=1}^{C} e^{z_j - m} \\right), \\quad m = \\max_j z_j\n",
    "$$  \n",
    "$$\n",
    "CE(z, y^*) = LSE(z) - z_{y^*}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**4) Batch Loss (N samples)**  \n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{N} \\sum_{n=1}^{N} \\big(-\\log p^{(n)}_{y^*(n)}\\big)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**5) Gradient w.r.t logits**  \n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z_i} = p_i - y_i\n",
    "$$  \n",
    "For a batch (mean reduction):  \n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z_{n,i}} = \\frac{1}{N}\\big(p_{n,i} - y_{n,i}\\big)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0fe13d-a725-4545-93cf-837e98cfa6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd31d9ea-5975-4f4e-b406-af597e6f75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def softmax_logits(logits, axis= -1):\n",
    "    z = logits - np.max(logits, axis=axis, keepdims=True)\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z, axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99794e6d-5f0c-4827-9672-3ae8956e1d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf6cdd5-5244-4294-9cdc-3f380215c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_from_probs(p, y, eps=1e-7, reduction='mean'):\n",
    "    \n",
    "    p_clamped = np.clip(p, eps, 1.0) #avoid log(0)\n",
    "    loss = -(np.sum(y * np.log(p_clamped),axis= -1))\n",
    "    if reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    else:\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664bdbbc-b260-404c-8e5d-1a44e6312751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b49f6bf8-e361-4ad5-9aff-ddd7fed2afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_from_logits(z, y, reduction='mean'):\n",
    "    max_z = np.max(logits, axis=-1, keepdims=True)\n",
    "    lse = np.log(np.sum(np.exp(logits - max_z), axis=-1, keepdims=True)) + max_z\n",
    "    # CE = LSE - z_true\n",
    "    z_true = np.sum(y * logits, axis=-1, keepdims=True)\n",
    "    loss = (lse - z_true).squeeze(-1)\n",
    "\n",
    "    if reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    else:\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98925d-cbe4-40d2-931a-4677e1f25831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21831f54-7402-4f57-ab76-d473b5ef412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_logits_cross_entropy(logits, y, reduction= 'mean'):\n",
    "    p = softmax_logits(logits, axis=-1)\n",
    "    grad = p - y\n",
    "    if reduction == 'mean':\n",
    "        N = logits.shape[0]\n",
    "        grad = grad / N\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c146e2f-ae72-4544-9893-b61034eb9f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e85352d-c990-4dfe-99a8-c7fc291c3ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "logits = np.array([[2.0, 0.5, -1.0, 0.0],\n",
    "                   [0.3, 1.2, 0.1, -0.5],\n",
    "                   [3.0, 2.0, 0.0, -1.0]])\n",
    "\n",
    "y_idx = np.array([0, 1, 0])\n",
    "\n",
    "# One-hot labels\n",
    "y = np.eye(4)[y_idx]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdb6770-c178-4a5b-bf68-5d88f3507c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e10696b-6c25-4a9f-a8a3-4f3691b4b077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs (rows sum to 1):\n",
      " [[0.71009992 0.15844471 0.03535379 0.09610157]\n",
      " [0.21152101 0.52025773 0.17317875 0.09504251]\n",
      " [0.69638749 0.25618664 0.03467109 0.01275478]]\n",
      "\n",
      "CE from probs: 0.45254319508077606\n",
      "\n",
      "CE from logits (stable): 0.45254319508077595\n"
     ]
    }
   ],
   "source": [
    "# Probabilities and loss\n",
    "p = softmax_logits(logits)\n",
    "loss_probs = cross_entropy_from_probs(p, y, reduction=\"mean\")\n",
    "loss_logits = cross_entropy_from_logits(logits, y, reduction=\"mean\")\n",
    "\n",
    "print(\"probs (rows sum to 1):\\n\", p)\n",
    "print(\"\\nCE from probs:\", loss_probs)\n",
    "print(\"\\nCE from logits (stable):\", loss_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614fa3b8-dd47-4871-aedc-89d757eb0a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "094ff8d8-8859-43e3-95bf-c51bde7a8860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad shape: (3, 4)\n",
      "row sums of grad (mean reduction): [2.77555756e-17 0.00000000e+00 1.56125113e-17]\n"
     ]
    }
   ],
   "source": [
    "# Gradient wrt logits\n",
    "g = grad_logits_cross_entropy(logits, y, reduction=\"mean\")\n",
    "print(\"grad shape:\", g.shape)\n",
    "print(\"row sums of grad (mean reduction):\", np.sum(g, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9653d11-1810-4961-86d0-79d4455275b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-dev)",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
