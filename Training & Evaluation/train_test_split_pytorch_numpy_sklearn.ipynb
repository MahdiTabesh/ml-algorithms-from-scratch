{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b77dad-a962-4f00-bb36-f241bb677475",
   "metadata": {},
   "source": [
    "# Train/Test Split & Shuffling\n",
    "\n",
    "In machine learning, we need to evaluate whether a model can **generalize** to unseen data.  \n",
    "To do this, we prepare the dataset by:\n",
    "\n",
    "1. **Splitting the data**  \n",
    "   - **Training set** → used to learn model parameters.  \n",
    "   - **Testing set** → kept unseen until final evaluation.  \n",
    "   - Common ratios: **80/20** or **70/30** (sometimes 60/20/20 with a validation set).\n",
    "\n",
    "2. **Shuffling the data**  \n",
    "   - Prevents bias when data is ordered (e.g., sorted labels, time sequence).  \n",
    "   - Ensures both train and test sets represent the overall dataset fairly.\n",
    "\n",
    "3. **Outcome**  \n",
    "   - Training set → guides learning.  \n",
    "   - Test set → provides an unbiased estimate of model performance.  \n",
    "\n",
    "> This step ensures that our model evaluation reflects **true generalization ability**, not just memorization of the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb95267-7c7a-42cc-a1e1-82ed5654bd90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaed5e2a-bb00-463d-81a1-305700a7d085",
   "metadata": {},
   "source": [
    "### Train/Test Split with NumPy\n",
    "Here we use NumPy to shuffle indices and split the dataset into 80% training and 20% testing sets.  \n",
    "This ensures that both sets are randomly sampled and representative of the whole dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaf6c060-3963-40bf-b91a-a28eca1941b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (40, 2)  Test shape: (10, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dummy dataset\n",
    "X = np.arange(100).reshape(50, 2)\n",
    "y = np.arange(50)\n",
    "\n",
    "# Shuffle indices\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Train/test split (80/20)\n",
    "split = int(0.8 * len(X))\n",
    "train_idx, test_idx = indices[:split], indices[split:]\n",
    "\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \" Test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c2769b-58ea-482b-9e5d-760b63dd8f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bdf3647-7292-4b1d-8298-a78a478f6769",
   "metadata": {},
   "source": [
    "### Train/Test Split with Scikit-learn\n",
    "Scikit-learn provides a convenient `train_test_split` function that handles both shuffling and splitting in one step.  \n",
    "We use `test_size=0.2` for an 80/20 split and fix a `random_state` for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8c7120-0740-4cae-973c-89ee8a79374e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (40, 2)  Test shape: (10, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.arange(100).reshape(50, 2)\n",
    "y = np.arange(50)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \" Test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf25a3f9-abe3-48a3-b0b4-96ca70590331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36416940-3822-4949-8d55-d5c49fb95692",
   "metadata": {},
   "source": [
    "### Train/Test Split with PyTorch\n",
    "In PyTorch, we wrap the data into a `TensorDataset` and then use `random_split` to divide it into training and test sets.  \n",
    "Finally, we create `DataLoader`s to iterate through mini-batches during training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15074c2d-1f1f-4708-a5d8-f291c81864ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 5  Test batches: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "X = torch.arange(100).reshape(50, 2).float()\n",
    "y = torch.arange(50)\n",
    "\n",
    "# Combine X and y into a dataset\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "# Train/test split sizes\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders (shuffle training set)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(\"Train batches:\", len(train_loader), \" Test batches:\", len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52dd21b-1851-46de-ab30-27b8e202bc22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48bc1279-2618-446f-a1e0-141c5f7cfa6e",
   "metadata": {},
   "source": [
    "### Real-World Example: Titanic Dataset (Train/Test Split)\n",
    "\n",
    "In this section, we apply train/test splitting to a real dataset — the **Titanic survival dataset**.  \n",
    "Steps:\n",
    "1. Load the dataset from `seaborn`.\n",
    "2. Select useful features (`age`, `fare`, `pclass`, `sex`) and the target (`survived`).\n",
    "3. Encode categorical data (`sex` → 0 for male, 1 for female).\n",
    "4. Perform an **80/20 split** using both:\n",
    "   - **Scikit-learn** → with `train_test_split`.\n",
    "   - **PyTorch** → with `TensorDataset` and `random_split`.\n",
    "5. Prepare PyTorch `DataLoader`s for batching during training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4590046b-bc34-4ae8-88fe-a664545e94ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (714, 4) (714,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "\n",
    "# 1) Load Titanic dataset from seaborn\n",
    "import seaborn as sns\n",
    "titanic = sns.load_dataset(\"titanic\")\n",
    "\n",
    "# 2) Keep only numeric + a few useful features (drop NaN for simplicity)\n",
    "df = titanic[[\"age\", \"fare\", \"pclass\", \"sex\", \"survived\"]].dropna()\n",
    "df[\"sex\"] = df[\"sex\"].map({\"male\": 0, \"female\": 1})  # encode categorical\n",
    "\n",
    "X = df.drop(\"survived\", axis=1).values\n",
    "y = df[\"survived\"].values\n",
    "\n",
    "print(\"Dataset shape:\", X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d471e74-1736-43b7-b1d9-148e3175c4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "    age     fare  pclass  sex  survived\n",
      "0  22.0   7.2500       3    0         0\n",
      "1  38.0  71.2833       1    1         1\n",
      "2  26.0   7.9250       3    1         1\n",
      "3  35.0  53.1000       1    1         1\n",
      "4  35.0   8.0500       3    0         0\n",
      "--------------------------------------\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 714 entries, 0 to 890\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       714 non-null    float64\n",
      " 1   fare      714 non-null    float64\n",
      " 2   pclass    714 non-null    int64  \n",
      " 3   sex       714 non-null    int64  \n",
      " 4   survived  714 non-null    int64  \n",
      "dtypes: float64(2), int64(3)\n",
      "memory usage: 33.5 KB\n",
      "None\n",
      "--------------------------------------\n",
      "\n",
      "Survival counts (target variable):\n",
      "survived\n",
      "0    424\n",
      "1    290\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show first 5 rows of the cleaned dataset\n",
    "print('--------------------------------------')\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "print('--------------------------------------')\n",
    "\n",
    "# Show info about columns (types + non-null counts)\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print('--------------------------------------')\n",
    "\n",
    "\n",
    "# Check survival distribution\n",
    "print(\"\\nSurvival counts (target variable):\")\n",
    "print(df[\"survived\"].value_counts())\n",
    "print('--------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "443c21f9-8b86-48af-9696-ee68bd079411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn split:\n",
      "Train: (571, 4)  Test: (143, 4)\n"
     ]
    }
   ],
   "source": [
    "# NumPy/Scikit-learn version\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Scikit-learn split:\")\n",
    "print(\"Train:\", X_train.shape, \" Test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "122d8077-6f83-4d8b-ae87-3cf8fdd46893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch split:\n",
      "Train batches: 36  Test batches: 9\n"
     ]
    }
   ],
   "source": [
    "# PyTorch version\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"PyTorch split:\")\n",
    "print(\"Train batches:\", len(train_loader), \" Test batches:\", len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646dae08-deb0-491f-aaad-d4ea7cdbe4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-dev)",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
