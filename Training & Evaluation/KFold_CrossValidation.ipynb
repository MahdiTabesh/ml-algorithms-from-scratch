{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqcBriyMY2PL"
   },
   "source": [
    "# Cross-Validation (K-Fold) with Dummy Dataset\n",
    "\n",
    "In this notebook, we explore **K-Fold Cross-Validation**, a robust technique for evaluating model performance by repeatedly training and testing on different partitions of the dataset. Instead of relying on a single train/test split, K-Fold validation divides the data into *K equal folds* and iteratively uses each fold as the test set while the remaining folds serve as the training set.\n",
    "\n",
    "**Why it matters:**\n",
    "- Reduces variance in evaluation results.\n",
    "- Utilizes all data for both training and validation.\n",
    "- Provides a better estimate of generalization performance.\n",
    "\n",
    "We'll:\n",
    "1. Generate a dummy dataset using `make_classification`.\n",
    "2. Implement K-Fold cross-validation using `sklearn.model_selection.KFold`.\n",
    "3. Train a simple model (e.g., Logistic Regression).\n",
    "4. Compute and interpret average accuracy across folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "doPX-8hvY3ni",
    "outputId": "173cc2da-bc9d-4461-9bab-7638c494fae8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold Accuracies: [0.925 0.975 0.875 0.875 0.925]\n",
      "Mean Accuracy: 0.9150 ± 0.0374\n"
     ]
    }
   ],
   "source": [
    "# K-Fold Cross Validation with Dummy Dataset\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Generate a dummy dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=200, n_features=5, n_informative=3,\n",
    "    n_redundant=0, n_classes=2, random_state=42\n",
    ")\n",
    "\n",
    "# Define the model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Define K-Fold Cross-Validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluate model using cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n",
    "\n",
    "print(\"Fold Accuracies:\", scores)\n",
    "print(f\"Mean Accuracy: {scores.mean():.4f} ± {scores.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alVzzjdXZGKo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5n9yOrNZOHh"
   },
   "source": [
    "# Pytorch Version: Manual K-Fold Cross-Validation\n",
    "\n",
    "In this section, we manually implement **K-Fold Cross-Validation** using PyTorch to understand the full training and evaluation workflow.\n",
    "\n",
    "**Goal:**  \n",
    "Perform repeated training and validation across K folds and average the results to obtain a more reliable estimate of the model’s performance.\n",
    "\n",
    "**Steps:**\n",
    "1. Create a dummy dataset using `sklearn.datasets.make_classification`.\n",
    "2. Convert it into PyTorch tensors.\n",
    "3. Split the dataset using `KFold`.\n",
    "4. Train a simple MLP model on each fold.\n",
    "5. Evaluate and report mean accuracy across folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MyOW7ONgZTbg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bB7s3hjUZXxL"
   },
   "outputs": [],
   "source": [
    "# Generate dummy dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=200, n_features=5, n_informative=3,\n",
    "    n_redundant=0, n_classes=2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Q3CVBYYOZYGH"
   },
   "outputs": [],
   "source": [
    "# Define a simple MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=5, hidden_dim=16, output_dim=2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9CBFF3h4ZYN0"
   },
   "outputs": [],
   "source": [
    "# Define training function\n",
    "def train_model(model, optimizer, criterion, X_train, y_train, epochs=50):\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_val)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = (preds == y_val).float().mean().item()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRA5KEg3ZQmL",
    "outputId": "e1217c3c-4a97-49d5-c2fb-873cf5cb99f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Accuracy: 0.9250\n",
      "Fold 2 Accuracy: 0.9750\n",
      "Fold 3 Accuracy: 0.8500\n",
      "Fold 4 Accuracy: 0.8750\n",
      "Fold 5 Accuracy: 0.9250\n",
      "\n",
      "Mean Accuracy: 0.9100 ± 0.0436\n"
     ]
    }
   ],
   "source": [
    "# K-Fold setup\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "\n",
    "# Loop over folds\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_tensor)):\n",
    "    X_train, X_val = X_tensor[train_idx], X_tensor[val_idx]\n",
    "    y_train, y_val = y_tensor[train_idx], y_tensor[val_idx]\n",
    "\n",
    "    # New model for each fold\n",
    "    model = MLP()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_model(model, optimizer, criterion, X_train, y_train)\n",
    "    acc = evaluate_model(model, X_val, y_val)\n",
    "    fold_accuracies.append(acc)\n",
    "\n",
    "    print(f\"Fold {fold + 1} Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Report overall performance\n",
    "print(f\"\\nMean Accuracy: {np.mean(fold_accuracies):.4f} ± {np.std(fold_accuracies):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95rjmMWfZxEv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
