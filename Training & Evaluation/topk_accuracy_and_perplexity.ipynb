{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psUALrrdQfTu"
   },
   "source": [
    "# Top-k Accuracy & Perplexity\n",
    "\n",
    "In this notebook, we explore two essential evaluation metrics used in modern ML and DL models:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Top-k Accuracy\n",
    "Top-k accuracy measures how often the correct label is among the model’s **top-k predicted classes**.\n",
    "- **Top-1 Accuracy** = standard accuracy.\n",
    "- **Top-5 Accuracy** = checks if the true label is within top 5 predictions.\n",
    "- Common in **image classification** tasks like ImageNet or CIFAR-100.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Top-k Accuracy} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}\\{y_i \\in \\text{TopK}(p_i, k)\\}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Perplexity\n",
    "Perplexity quantifies **how well a probabilistic model (usually a Language Model)** predicts a sequence of words.  \n",
    "It’s the exponential of the average negative log-likelihood:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity} = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i | w_{<i})\\right)\n",
    "$$\n",
    "\n",
    "- Lower perplexity → model is more confident and accurate.\n",
    "- Common in **language modeling, text generation**, and **speech recognition**.\n",
    "\n",
    "---\n",
    "\n",
    "## Goals of This Notebook\n",
    "- Implement `top_k_accuracy` from scratch using PyTorch & NumPy.  \n",
    "- Compute **Top-1** and **Top-5** accuracy on dummy data.  \n",
    "- Implement **Perplexity** for a sample language model output.  \n",
    "- Compare results and interpret their meaning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtM0EOPAQqsA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYUVWfonSLiv"
   },
   "source": [
    "# Top-k Implementation - Pytorch Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oQc7FgpER1J7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def topk_accuracy_simple(logits, targets, k=5):\n",
    "    \"\"\"\n",
    "    This function computes Top-k Accuracy (default k=5).\n",
    "\n",
    "    Inputs:\n",
    "      logits: model outputs shaped (N, C) where each row has the prediction scores for C classes.\n",
    "      targets: true labels shaped (N,).\n",
    "      k: how many top predictions to check.\n",
    "\n",
    "    Output:\n",
    "    Returns the percentage of samples where the correct label appears in the model’s top-k predictions.\n",
    "    \"\"\"\n",
    "    # Get the indices of the top k predictions for each sample\n",
    "    topk = torch.topk(logits, k, dim=1).indices\n",
    "\n",
    "    # Check if the true label is in the top-k predictions\n",
    "    correct = topk.eq(targets.view(-1, 1))\n",
    "\n",
    "    # Compute how many are correct\n",
    "    correct_count = correct.any(dim=1).sum().item()\n",
    "\n",
    "    # Convert to percentage\n",
    "    accuracy = 100.0 * correct_count / logits.size(0)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ok6z1qA-R1_g",
    "outputId": "f03648b0-906a-424a-d8a4-c238d2634832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 80.00%\n",
      "Top-3 Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Example: 5 samples, 4 classes\n",
    "logits = torch.tensor([\n",
    "    [2.5, 1.2, 0.3, 0.7],\n",
    "    [0.1, 2.1, 0.5, 0.2],\n",
    "    [0.8, 0.3, 2.9, 0.1],\n",
    "    [1.0, 0.5, 0.2, 3.0],\n",
    "    [0.2, 1.5, 2.2, 0.9]\n",
    "])\n",
    "targets = torch.tensor([0, 1, 2, 3, 1])\n",
    "\n",
    "# Compute Top-1 and Top-3 accuracy\n",
    "top1 = topk_accuracy_simple(logits, targets, k=1)\n",
    "top3 = topk_accuracy_simple(logits, targets, k=3)\n",
    "\n",
    "print(f\"Top-1 Accuracy: {top1:.2f}%\")\n",
    "print(f\"Top-3 Accuracy: {top3:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-9FjT2_SWEF"
   },
   "source": [
    "# Top-k Implementation - Numpy Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Hehrvvd-R84_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def topk_accuracy_simple_numpy(scores, targets, k=5):\n",
    "    \"\"\"\n",
    "    This function calculates Top-k accuracy using NumPy.\n",
    "\n",
    "    Inputs:\n",
    "      scores: an array (N, C) of model outputs (probabilities or logits).\n",
    "      targets: an array (N,) of true class labels.\n",
    "      k: number of top predictions to consider (default = 5).\n",
    "\n",
    "    Output:\n",
    "      Returns a single float accuracy value, showing the percentage of samples where the true label appears in the model’s top-k predictions.\n",
    "    \"\"\"\n",
    "    # Step 1: Find indices of the top k predictions for each sample\n",
    "    topk = np.argsort(scores, axis=1)[:, -k:]\n",
    "\n",
    "    # Step 2: Check if each true label is inside its top-k predictions\n",
    "    correct = np.any(topk == targets[:, None], axis=1)\n",
    "\n",
    "    # Step 3: Count how many are correct\n",
    "    accuracy = 100.0 * np.sum(correct) / len(targets)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGjDrOuySFGv",
    "outputId": "685b1672-e4e1-4761-f8c4-976889e1f156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 80.00%\n",
      "Top-3 Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Example: 5 samples, 4 classes\n",
    "scores = np.array([\n",
    "    [2.5, 1.2, 0.3, 0.7],\n",
    "    [0.1, 2.1, 0.5, 0.2],\n",
    "    [0.8, 0.3, 2.9, 0.1],\n",
    "    [1.0, 0.5, 0.2, 3.0],\n",
    "    [0.2, 1.5, 2.2, 0.9]\n",
    "])\n",
    "targets = np.array([0, 1, 2, 3, 1])\n",
    "\n",
    "# Compute Top-1 and Top-3 accuracy\n",
    "top1 = topk_accuracy_simple_numpy(scores, targets, k=1)\n",
    "top3 = topk_accuracy_simple_numpy(scores, targets, k=3)\n",
    "\n",
    "print(f\"Top-1 Accuracy: {top1:.2f}%\")\n",
    "print(f\"Top-3 Accuracy: {top3:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVogdFGkScjA"
   },
   "source": [
    "# Perplexity - Pytorch Version\n",
    "\n",
    "Perplexity = “how confused is the model on average per token?”\n",
    "\n",
    "Mathematically: ppl = exp(average negative log-likelihood).\n",
    "\n",
    "- Lower ppl → better (model is more confident/correct).\n",
    "\n",
    "- Typically computed for language models on next-token prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q7Yjvd4NSHfb",
    "outputId": "2b69657a-6696-4c06-8df0-8bf89449af3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch perplexity: 8.822303771972656\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def perplexity_from_logits_torch(logits, targets):\n",
    "    \"\"\"\n",
    "    This function computes perplexity from model logits.\n",
    "\n",
    "    Process:\n",
    "        Calculates per-token cross-entropy loss and then takes the exponential of the mean loss.\n",
    "\n",
    "    Output:\n",
    "        Returns a single float value (perplexity) — a measure of how well the model predicts the sequence (lower is better).\n",
    "    \"\"\"\n",
    "    B, T, V = logits.shape\n",
    "    loss_per_token = F.cross_entropy(\n",
    "        logits.view(-1, V),\n",
    "        targets.view(-1),\n",
    "        reduction=\"none\"\n",
    "    )\n",
    "    avg_nll = loss_per_token.mean()   # average negative log-likelihood\n",
    "    ppl = torch.exp(avg_nll).item()\n",
    "    return ppl\n",
    "\n",
    "# Tiny sanity check\n",
    "torch.manual_seed(0)\n",
    "B, T, V = 2, 4, 6\n",
    "logits = torch.randn(B, T, V)\n",
    "targets = torch.randint(0, V, (B, T))\n",
    "print(\"PyTorch perplexity:\", perplexity_from_logits_torch(logits, targets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oY3dpT4CS6vM"
   },
   "source": [
    "# Tip: If you already have your average cross-entropy loss (e.g., loss.item() from your training step), perplexity is just:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nXTwPhPYTAU7"
   },
   "outputs": [],
   "source": [
    "# ppl = float(torch.exp(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJZPC8WgTGx0"
   },
   "source": [
    "# (Optional) Mask padding in PyTorch (one extra line)\n",
    "\n",
    "If your sequences are padded with pad_token_id, ignore those tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6OELXt4GTDws"
   },
   "outputs": [],
   "source": [
    "def perplexity_from_logits_torch_masked(logits, targets, pad_token_id):\n",
    "    B, T, V = logits.shape\n",
    "    loss_per_token = F.cross_entropy(\n",
    "        logits.view(-1, V),\n",
    "        targets.view(-1),\n",
    "        reduction=\"none\"\n",
    "    )\n",
    "    mask = (targets.view(-1) != pad_token_id)\n",
    "    avg_nll = (loss_per_token[mask]).mean()\n",
    "    return float(torch.exp(avg_nll))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPm0X9fHTNcn"
   },
   "source": [
    "# Perplexity (from probabilities or logits) - Numpy Version\n",
    "\n",
    "- A) If you have probabilities for the true tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FzGFaJ2oTLoA",
    "outputId": "a4efb379-eb0c-49c9-b4a3-439d106f604f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy ppl (from probs): 3.443299437293505\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def perplexity_from_probs_numpy(true_token_probs):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        true_token_probs: 1D array of length N with the model's probability\n",
    "                          assigned to the correct token at each step (0< p <=1)\n",
    "    Returns:\n",
    "        float perplexity\n",
    "    \"\"\"\n",
    "    # avoid log(0)\n",
    "    eps = 1e-12\n",
    "    nll = -np.log(true_token_probs + eps)   # per-token negative log-likelihood\n",
    "    ppl = float(np.exp(nll.mean()))\n",
    "    return ppl\n",
    "\n",
    "# Example with fake probs for 6 tokens\n",
    "p = np.array([0.3, 0.1, 0.25, 0.8, 0.5, 0.2])\n",
    "print(\"NumPy ppl (from probs):\", perplexity_from_probs_numpy(p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iS2wGwCQTjp8"
   },
   "source": [
    "## B) If you have logits (stable softmax inside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-6Wq5A12ThG5",
    "outputId": "0b45ac96-4fae-42bb-f535-7efbb4508d39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy ppl (from logits): 4.743589836038051\n"
     ]
    }
   ],
   "source": [
    "def softmax_numpy(x, axis=-1):\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    e = np.exp(x - x_max)\n",
    "    return e / np.sum(e, axis=axis, keepdims=True)\n",
    "\n",
    "def perplexity_from_logits_numpy(logits, targets):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits : (B, T, V) numpy array of unnormalized scores\n",
    "        targets: (B, T)   numpy int array of true token ids\n",
    "    Returns:\n",
    "        float perplexity\n",
    "    \"\"\"\n",
    "    B, T, V = logits.shape\n",
    "    probs = softmax_numpy(logits, axis=-1)\n",
    "    # pick probability of the true token at each position\n",
    "    rows = np.arange(B)[:, None]\n",
    "    cols = np.arange(T)[None, :]\n",
    "    true_probs = probs[rows, cols, targets]\n",
    "    return perplexity_from_probs_numpy(true_probs.ravel())\n",
    "\n",
    "# Tiny example\n",
    "np.random.seed(0)\n",
    "B, T, V = 2, 4, 6\n",
    "logits_np = np.random.randn(B, T, V)\n",
    "targets_np = np.random.randint(0, V, size=(B, T))\n",
    "print(\"NumPy ppl (from logits):\", perplexity_from_logits_numpy(logits_np, targets_np))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_0_OSaYlTnk4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
