{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b1d8a5-0fe2-4956-bf4f-f4090b0a67a4",
   "metadata": {},
   "source": [
    "# Tiny Training Loop (Manual Dataset + Mini-Batches)\n",
    "\n",
    "**Goal:** Implement a minimal end-to-end training loop *from scratch* using NumPy:\n",
    "- Hand-craft a tiny 2-D binary classification dataset\n",
    "- Logistic regression model: \\\\( \\hat y = \\sigma(XW + b) \\\\)\n",
    "- Binary cross-entropy loss (BCE)\n",
    "- Mini-batch stochastic gradient descent (SGD) with shuffling each epoch\n",
    "- Print loss/accuracy to verify learning\n",
    "\n",
    "**Key formulas**\n",
    "\n",
    "- Sigmoid: \\\\( \\sigma(z) = 1 / (1 + e^{-z}) \\\\)\n",
    "- BCE: $\\mathcal{L} = -\\frac{1}{B}\\sum \\big[ y \\log \\hat{y} + (1-y)\\log(1-\\hat{y}) \\big]$\n",
    "- Gradients (batch of size B):\n",
    "  - Error: $e = \\hat{y} - y$\n",
    "  - $\\nabla_W = \\frac{1}{B} X^\\top e$\n",
    "  - $\\nabla_b = \\frac{1}{B} \\sum e$\n",
    "\n",
    "\n",
    "**Checklist**\n",
    "- [ ] Build manual dataset\n",
    "- [ ] Implement forward pass, loss, grads\n",
    "- [ ] Write a mini-batch iterator (with shuffling)\n",
    "- [ ] Train and print metrics\n",
    "- [ ] (Optional) Try different batch sizes / learning rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd1c1907-2317-4fd4-842e-5c93d5d117b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# 1) Manual toy dataset (2-D)\n",
    "# -----------------------------\n",
    "# Two Gaussian blobs\n",
    "n_per_class = 100\n",
    "mean0, mean1 = np.array([-1.5, -1.0]), np.array([1.2, 1.3])\n",
    "cov = np.array([[0.5, 0.0],\n",
    "                [0.0, 0.5]])\n",
    "\n",
    "X0 = np.random.multivariate_normal(mean0, cov, size=n_per_class)\n",
    "X1 = np.random.multivariate_normal(mean1, cov, size=n_per_class)\n",
    "\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.concatenate([np.zeros(n_per_class), np.ones(n_per_class)])\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# Standardize features\n",
    "X_mean, X_std = X.mean(axis=0, keepdims=True), X.std(axis=0, keepdims=True) + 1e-8\n",
    "X = (X - X_mean) / X_std\n",
    "\n",
    "\n",
    "# 2) Model params (logistic regression)\n",
    "# ---------------------------------------\n",
    "W = np.random.randn(n_features, 1) * 0.01\n",
    "b = np.zeros((1,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2c5964b-1866-44be-9932-f7686e8a743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Helper functions\n",
    "# -----------------------------\n",
    "def sigmoid(z):\n",
    "    # Stable sigmoid\n",
    "    z = np.clip(z, -20, 20)\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def bce_loss(y_true, y_prob, eps=1e-8):\n",
    "    y_prob = np.clip(y_prob, eps, 1 - eps)\n",
    "    return -np.mean(y_true * np.log(y_prob) + (1 - y_true) * np.log(1 - y_prob))\n",
    "\n",
    "def accuracy(y_true, y_prob):\n",
    "    y_pred = (y_prob >= 0.5).astype(np.float32)\n",
    "    return (y_pred == y_true).mean()\n",
    "\n",
    "def iter_minibatches(X, y, batch_size, shuffle=True):\n",
    "    n = X.shape[0]\n",
    "    idx = np.arange(n)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    for start in range(0, n, batch_size):\n",
    "        end = start + batch_size\n",
    "        batch_idx = idx[start:end]\n",
    "        yield X[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64e0852a-e029-4bc7-9274-72ecaed0c3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss=0.3857 | acc=99.5%\n",
      "Epoch 05 | loss=0.1447 | acc=99.5%\n",
      "Epoch 10 | loss=0.0905 | acc=99.5%\n",
      "Epoch 15 | loss=0.0697 | acc=99.5%\n",
      "Epoch 20 | loss=0.0584 | acc=99.5%\n",
      "Epoch 25 | loss=0.0512 | acc=99.5%\n",
      "Epoch 30 | loss=0.0462 | acc=99.5%\n",
      "\n",
      "W: [2.47450664 2.20518569]  b: -0.02075615803662676\n"
     ]
    }
   ],
   "source": [
    "# 4) Training loop (mini-batch SGD)\n",
    "# ---------------------------------\n",
    "lr = 0.1\n",
    "batch_size = 16\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # one epoch of mini-batch SGD\n",
    "    for Xb, yb in iter_minibatches(X, y, batch_size=batch_size, shuffle=True):\n",
    "        # Forward\n",
    "        logits = Xb @ W + b\n",
    "        yhat = sigmoid(logits)\n",
    "\n",
    "        # Loss\n",
    "        loss = bce_loss(yb, yhat)\n",
    "\n",
    "        # Backprop (gradients)\n",
    "        # error = dL/dlogits = (yhat - y)\n",
    "        error = (yhat - yb)\n",
    "        grad_W = (Xb.T @ error) / Xb.shape[0]\n",
    "        grad_b = error.mean(axis=0)\n",
    "\n",
    "        # SGD update\n",
    "        W -= lr * grad_W\n",
    "        b -= lr * grad_b\n",
    "\n",
    "    # end of epoch: report metrics on full data\n",
    "    with np.errstate(over='ignore'):\n",
    "        yhat_all = sigmoid(X @ W + b)\n",
    "    train_loss = bce_loss(y, yhat_all)\n",
    "    train_acc = accuracy(y, yhat_all)\n",
    "    if epoch % 5 == 0 or epoch == 1 or epoch == epochs:\n",
    "        print(f\"Epoch {epoch:02d} | loss={train_loss:.4f} | acc={train_acc*100:.1f}%\")\n",
    "\n",
    "# Quick check\n",
    "print(\"\\nW:\", W.ravel(), \" b:\", float(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcba8c8-2bb1-45cc-8a29-af8a6567bcd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ceb0d995-7c3e-4bd5-bc39-352a0f09a822",
   "metadata": {},
   "source": [
    "# Tiny Training Loop with PyTorch — Custom `Dataset` + Mini-batches\n",
    "\n",
    "**Goals**\n",
    "- Create a manual toy dataset and wrap it in a PyTorch `Dataset`\n",
    "- Use `DataLoader` for shuffling + mini-batches\n",
    "- Implement a clean, reusable training loop (works for any model)\n",
    "- Baseline: Logistic Regression on 2-D points\n",
    "- (Optional) Tiny Transformer for sequence classification on dummy tokens\n",
    "\n",
    "**Key Ideas**\n",
    "- `Dataset.__len__` and `Dataset.__getitem__` make your data iterable.\n",
    "- `DataLoader(..., batch_size, shuffle=True)` gives mini-batches each epoch.\n",
    "- Use `model.train()`/`model.eval()` and disable grads during evaluation.\n",
    "- Keep the loop generic: `train_one_epoch`, `evaluate`, reused across models.\n",
    "\n",
    "**What to try next**\n",
    "- Change `batch_size`, `lr`, and `epochs`\n",
    "- Swap the baseline for the Transformer block and train on toy sequences\n",
    "- Add early stopping / LR scheduler tomorrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46beed96-2346-4a23-aa6f-3e61aca80281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch tiny training loop with a custom Dataset and two model options\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e73d5c6a-999d-4713-929b-e488d3bc90ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) DATASETS\n",
    "class PointsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    2-D binary classification dataset: two Gaussian blobs (manual data).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_per_class=200, mean0=(-1.5, -1.0), mean1=(1.2, 1.3), cov=0.5, normalize=True):\n",
    "        self.X, self.y = self._make_blobs(n_per_class, mean0, mean1, cov)\n",
    "        if normalize:\n",
    "            m = self.X.mean(axis=0, keepdims=True)\n",
    "            s = self.X.std(axis=0, keepdims=True) + 1e-8\n",
    "            self.X = (self.X - m) / s\n",
    "        self.X = torch.from_numpy(self.X).float()\n",
    "        self.y = torch.from_numpy(self.y).float()\n",
    "\n",
    "    def _make_blobs(self, n, mean0, mean1, cov):\n",
    "        cov_mat = np.array([[cov, 0.0],[0.0, cov]])\n",
    "        X0 = np.random.multivariate_normal(np.array(mean0), cov_mat, size=n)\n",
    "        X1 = np.random.multivariate_normal(np.array(mean1), cov_mat, size=n)\n",
    "        X = np.vstack([X0, X1])\n",
    "        y = np.concatenate([np.zeros(n), np.ones(n)])\n",
    "        # quick shuffle for randomness\n",
    "        idx = np.random.permutation(len(X))\n",
    "        return X[idx], y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class ToySequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dummy sequence classification for a tiny Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, n_samples=2000, seq_len=16, vocab_size=50):\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        xs, ys = [], []\n",
    "        for _ in range(n_samples):\n",
    "            tokens = np.random.randint(1, vocab_size, size=seq_len, dtype=np.int64)\n",
    "            label = (tokens.sum() % 2).astype(np.int64)\n",
    "            xs.append(tokens)\n",
    "            ys.append(label)\n",
    "        self.X = torch.from_numpy(np.stack(xs))\n",
    "        self.y = torch.from_numpy(np.array(ys))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ef1e247-f3ff-4da2-ad1e-1aeb34864d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) MODELS\n",
    "class LogisticRegressor(nn.Module):\n",
    "    \"\"\"Baseline linear classifier for 2-D points (with BCEWithLogitsLoss).\"\"\"\n",
    "    def __init__(self, in_dim=2):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 2)\n",
    "        logits = self.linear(x)\n",
    "        return logits.squeeze(-1)\n",
    "\n",
    "\n",
    "class TinyTransformerClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal Transformer encoder for sequence classification.\n",
    "    - Token embedding + positional encoding\n",
    "    - TransformerEncoder with a couple of layers/heads\n",
    "    - CLS-style pooling: use the first token’s hidden state for classification\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=50, d_model=64, nhead=4, num_layers=2, dim_ff=128, max_len=256, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_ff, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        self.cls = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # tokens: (B, L) longs\n",
    "        B, L = tokens.shape\n",
    "        pos = torch.arange(L, device=tokens.device).unsqueeze(0).expand(B, L)\n",
    "        x = self.tok_emb(tokens) + self.pos_emb(pos)\n",
    "        h = self.encoder(x)\n",
    "        cls_h = h[:, 0, :]\n",
    "        logits = self.cls(cls_h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42849373-1c84-4a79-a77d-25153eba2829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) TRAIN / EVAL UTILITIES\n",
    "def accuracy_from_logits_binary(logits, targets):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs >= 0.5).float()\n",
    "    return (preds == targets).float().mean().item()\n",
    "\n",
    "def accuracy_from_logits_multiclass(logits, targets):\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    return (preds == targets).float().mean().item()\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device, task=\"binary\"):\n",
    "    model.train()\n",
    "    running_loss, running_acc, n_batches = 0.0, 0.0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if task == \"binary\":\n",
    "            running_acc += accuracy_from_logits_binary(logits.detach(), yb.detach())\n",
    "        else:\n",
    "            running_acc += accuracy_from_logits_multiclass(logits.detach(), yb.detach())\n",
    "        n_batches += 1\n",
    "    return running_loss / n_batches, running_acc / n_batches\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device, task=\"binary\"):\n",
    "    model.eval()\n",
    "    running_loss, running_acc, n_batches = 0.0, 0.0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        running_loss += loss.item()\n",
    "        if task == \"binary\":\n",
    "            running_acc += accuracy_from_logits_binary(logits, yb)\n",
    "        else:\n",
    "            running_acc += accuracy_from_logits_multiclass(logits, yb)\n",
    "        n_batches += 1\n",
    "    return running_loss / n_batches, running_acc / n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18dd52c7-754c-4e67-87ec-d95f2f9d0c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training: Logistic Regression on 2-D Points ===\n",
      "Epoch 01 | train loss 0.2953 acc 99.4% | val loss 0.2815 acc 98.8%\n",
      "Epoch 05 | train loss 0.1522 acc 100.0% | val loss 0.1683 acc 98.8%\n",
      "Epoch 10 | train loss 0.1017 acc 100.0% | val loss 0.1227 acc 98.8%\n",
      "Epoch 20 | train loss 0.0664 acc 100.0% | val loss 0.0890 acc 98.8%\n",
      "Epoch 30 | train loss 0.0517 acc 100.0% | val loss 0.0747 acc 98.8%\n"
     ]
    }
   ],
   "source": [
    "# 4) EXPERIMENT A — Points + Logistic Regression\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "points_ds = PointsDataset(n_per_class=200, normalize=True)\n",
    "n_total = len(points_ds)\n",
    "n_train = int(0.8 * n_total)\n",
    "n_val = n_total - n_train\n",
    "train_ds, val_ds = torch.utils.data.random_split(points_ds, [n_train, n_val], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=128, shuffle=False)\n",
    "\n",
    "model_lr = LogisticRegressor(in_dim=2).to(device)\n",
    "criterion_bce = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model_lr.parameters(), lr=0.1)\n",
    "\n",
    "print(\"\\n=== Training: Logistic Regression on 2-D Points ===\")\n",
    "for epoch in range(1, 31):\n",
    "    tr_loss, tr_acc = train_one_epoch(model_lr, train_loader, criterion_bce, optimizer, device, task=\"binary\")\n",
    "    va_loss, va_acc = evaluate(model_lr, val_loader, criterion_bce, device, task=\"binary\")\n",
    "    if epoch in {1, 5, 10, 20, 30}:\n",
    "        print(f\"Epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc*100:.1f}% | val loss {va_loss:.4f} acc {va_acc*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb62657-d87d-4772-86da-22d56267c7d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-dev)",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
