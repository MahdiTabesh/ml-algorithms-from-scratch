{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bV3zuIW3DVNR"
   },
   "source": [
    "# F1 Score, ROC, and AUC (Complete Overview)\n",
    "\n",
    "---\n",
    "\n",
    "## Confusion Matrix & Core Metrics\n",
    "\n",
    "Let \\(TP, FP, TN, FN\\) be the counts for a binary classifier.\n",
    "\n",
    "- **Precision:**  \n",
    "  $$ \\text{Precision} = \\frac{TP}{TP + FP} $$\n",
    "- **Recall / True Positive Rate (TPR):**  \n",
    "  $$ \\text{Recall} = \\text{TPR} = \\frac{TP}{TP + FN} $$\n",
    "- **False Positive Rate (FPR):**  \n",
    "  $$ \\text{FPR} = \\frac{FP}{FP + TN} $$\n",
    "\n",
    "---\n",
    "\n",
    "## 1. F1 Score — Balancing Precision and Recall\n",
    "\n",
    "### In Simple Words:\n",
    "F1 score tells you **how good your model is at finding positives** (like detecting spam emails, diseases, or frauds) **without making too many mistakes**.  \n",
    "It combines both **precision** (how many predicted positives are correct) and **recall** (how many real positives you caught).\n",
    "\n",
    "Think of:\n",
    "- **Precision** → “When I say this is spam, how often am I right?”  \n",
    "- **Recall** → “Of all the real spam, how many did I catch?”  \n",
    "- **F1** → “How well am I balancing the two?”\n",
    "\n",
    "It’s the **harmonic mean** of precision and recall — meaning if either one is low, F1 will also drop.\n",
    "\n",
    "### Formula:\n",
    "$$\n",
    "F_1 = 2\\cdot\\frac{\\text{Precision}\\cdot \\text{Recall}}\n",
    "{\\text{Precision} + \\text{Recall}}\n",
    "= \\frac{2TP}{2TP + FP + FN}.\n",
    "$$\n",
    "\n",
    "More generally,\n",
    "$$\n",
    "F_\\beta = (1+\\beta^2)\\cdot \\frac{\\text{Precision}\\cdot \\text{Recall}}\n",
    "{\\beta^2\\cdot \\text{Precision} + \\text{Recall}},\n",
    "$$\n",
    "\n",
    "where \\(\\beta>1\\) emphasizes **recall** and \\(\\beta<1\\) emphasizes **precision**.\n",
    "\n",
    "> **Note:** F1 is computed *after* converting probabilities to **hard labels** using a **threshold** (often not 0.5).  \n",
    "> You can tune this threshold on validation data to get the best trade-off.\n",
    "\n",
    "### Applications:\n",
    "- **Medical tests:** Catch diseases (high recall) without too many false alarms.  \n",
    "- **Spam or fraud detection:** Deal with imbalanced data; balance recall vs. false positives.  \n",
    "- **Search systems / recommendation:** Ensure retrieved items are both relevant and complete.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. ROC Curve — Visualizing Trade-Offs\n",
    "\n",
    "### In Simple Words:\n",
    "The **ROC curve** shows how your model’s performance changes when you move the decision **threshold** up and down.\n",
    "\n",
    "- Lowering the threshold → **catch more positives** (higher recall / TPR)  \n",
    "  but also → **more false alarms** (higher FPR).  \n",
    "- Increasing the threshold → **fewer false alarms**,  \n",
    "  but also → **miss more real positives.**\n",
    "\n",
    "The ROC curve plots:\n",
    "- **Y-axis:** True Positive Rate (TPR = Recall)\n",
    "- **X-axis:** False Positive Rate (FPR)\n",
    "\n",
    "This lets you **see how well your model separates the classes** (good vs. bad, positive vs. negative) **independent of any specific threshold**.\n",
    "\n",
    "### Formula:\n",
    "- **TPR:**  \n",
    "  $$ \\text{TPR} = \\frac{TP}{TP+FN} $$\n",
    "- **FPR:**  \n",
    "  $$ \\text{FPR} = \\frac{FP}{FP+TN} $$\n",
    "\n",
    "### When to Use ROC:\n",
    "- To **compare models visually** across thresholds.  \n",
    "- When you care about **ranking** ability, not just binary accuracy.  \n",
    "- To check whether the model truly distinguishes between classes.\n",
    "\n",
    "### Examples:\n",
    "- Comparing two medical models to see which better separates healthy vs. sick patients.  \n",
    "- Evaluating credit risk models — higher ROC = better separation between safe and risky clients.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. AUC — Area Under the ROC Curve\n",
    "\n",
    "### In Simple Words:\n",
    "AUC is a **single number summary** of the ROC curve.  \n",
    "It represents the **probability that your model ranks a random positive higher than a random negative**.\n",
    "\n",
    "- **AUC ≈ 1.0** → near-perfect model  \n",
    "- **AUC ≈ 0.5** → random guessing  \n",
    "- **AUC < 0.5** → model is worse than random (reversed ordering)\n",
    "\n",
    "### Formula:\n",
    "$$\n",
    "\\text{AUC} = \\int_0^1 \\text{TPR}(\\text{FPR})\\, d(\\text{FPR})\n",
    "$$\n",
    "\n",
    "### Applications:\n",
    "- **Ranking tasks:** Fraud detection, credit scoring, medical triage.  \n",
    "- **When you want one metric** to summarize how well the model separates classes.  \n",
    "- Great for **imbalanced datasets** where accuracy can be misleading.\n",
    "\n",
    "---\n",
    "\n",
    "## ROC vs. PR Curve (When Positives Are Rare)\n",
    "- **ROC curves** can look overly optimistic if positives are very rare.  \n",
    "- In such cases, prefer **Precision–Recall (PR) curves** and **AUPRC**, which focus on positive detection quality.\n",
    "\n",
    "---\n",
    "\n",
    "## Multiclass Extensions\n",
    "- **F1:** compute per class (one-vs-rest), then average:  \n",
    "  - **Macro** (unweighted mean)  \n",
    "  - **Weighted** (weighted by class frequency)  \n",
    "  - **Micro** (global TP/FP/FN)  \n",
    "- **ROC/AUC:** one-vs-rest per class, then macro/micro average.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Use Cases\n",
    "| Problem Type | Goal | Best Metric |\n",
    "|---------------|------|--------------|\n",
    "| **Medical screening / safety** | Catch all positives (high recall) | $F_1$, $F_\\beta$ with $\\beta>1$ |\n",
    "| **Fraud / spam detection** | Handle rare positives | PR Curve, AUPRC, F1 |\n",
    "| **Ranking / triage systems** | Measure separability | ROC & AUC |\n",
    "| **Alert systems (fixed capacity)** | Balance false alarms vs misses | Tune threshold, track F1 |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "1. Use **scores (not hard labels)** to compute **ROC** and **AUC**.  \n",
    "2. **Threshold choice** heavily affects **F1** — always tune it on validation data.  \n",
    "3. For **imbalanced** datasets, prioritize **PR/AUPRC** alongside F1.  \n",
    "4. For **multiclass**, report **macro/micro/weighted** averages for clarity and fairness.  \n",
    "5. Use **AUC** when you want a **threshold-free comparison**, and **F1** when you must output binary decisions.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYVyCxtc74UV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hP8FHtcBCeb_"
   },
   "source": [
    "### Create a Toy Dataset  \n",
    "We generate a small synthetic dataset with `y_true` labels and model prediction `scores`.  \n",
    "This dataset will be used for all our F1, ROC, and AUC calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z48rHsQn-UAB",
    "outputId": "ac1712d5-f14c-4f79-cebd-8e0d9554a41d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_ture distribution: [140  60]\n",
      "scores range: 0.03497046698601967 to 0.8789788976876499\n"
     ]
    }
   ],
   "source": [
    "# create a small reproducible toy dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "n = 200\n",
    "\n",
    "y_true = (rng.random(n) < 0.30).astype(int)\n",
    "\n",
    "scores = rng.beta(a = 2+2*y_true, b = 5 -2*y_true, size= n)\n",
    "\n",
    "print(\"y_ture distribution:\", np.bincount((y_true)))\n",
    "\n",
    "print(\"scores range:\", float(scores.min()), 'to', float(scores.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xjS5nMZCh3o"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ms_QfeoCinA"
   },
   "source": [
    "### Cell 2 — Compute F1 Score from Scratch  \n",
    "We define helper functions to calculate `TP, FP, TN, FN` at a given threshold  \n",
    "and then compute `Precision`, `Recall`, and `F1` manually using NumPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4qV1MsUD_Zzz",
    "outputId": "4acea9de-6234-4a62-9a2e-1ad2ad97e10f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold=0.50 -> TP=41, FP=19, TN=121, FN=19\n",
      "Precision=0.683, Recall=0.683, F1=0.683\n"
     ]
    }
   ],
   "source": [
    "# Confusion counts, Precision, Recall, F1 at a given threshold\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def confusion_counts_from_threshold(y_true: np.ndarray, scores: np.ndarray, threshold: float):\n",
    "    \"\"\"\n",
    "    Convert scores to hard labels using threshold, then compute TP, FP, TN, FN.\n",
    "    \"\"\"\n",
    "    y_pred = (scores >= threshold).astype(int)\n",
    "    TP = int(((y_pred == 1) & (y_true == 1)).sum())\n",
    "    FP = int(((y_pred == 1) & (y_true == 0)).sum())\n",
    "    TN = int(((y_pred == 0) & (y_true == 0)).sum())\n",
    "    FN = int(((y_pred == 0) & (y_true == 1)).sum())\n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "def precision_recall_f1_from_counts(TP: int, FP: int, TN: int, FN: int, eps: float = 1e-12):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, F1 from TP/FP/TN/FN. eps avoids division by zero.\n",
    "    \"\"\"\n",
    "    precision = TP / (TP + FP + eps)\n",
    "    recall    = TP / (TP + FN + eps)\n",
    "    f1        = 2 * precision * recall / (precision + recall + eps)\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Demo at an arbitrary threshold (e.g., 0.5)\n",
    "TP, FP, TN, FN = confusion_counts_from_threshold(y_true, scores, threshold=0.5)\n",
    "P, R, F1 = precision_recall_f1_from_counts(TP, FP, TN, FN)\n",
    "\n",
    "print(f\"Threshold=0.50 -> TP={TP}, FP={FP}, TN={TN}, FN={FN}\")\n",
    "print(f\"Precision={P:.3f}, Recall={R:.3f}, F1={F1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8zM7AfUCt00"
   },
   "source": [
    "### Build ROC Curve from Scratch  \n",
    "We sweep through all thresholds and compute pairs of `(FPR, TPR)` to draw the ROC curve.  \n",
    "This shows how the model performance changes with different classification thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ik3IhdIRCLth",
    "outputId": "12ba4c6d-b6d1-4c5d-d0e6-1a174c13c661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC points: 202 FPR[0..3]= [0. 0. 0.] TPR[0..3]= [0.         0.01666667 0.03333333] THR[0..3]= [       inf 0.8789789  0.85540335]\n"
     ]
    }
   ],
   "source": [
    "# ROC curve by sweeping thresholds\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def roc_curve_from_scores(y_true: np.ndarray, scores: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute ROC curve points (FPR, TPR, thresholds) by sweeping unique score values.\n",
    "    Returns arrays sorted by descending threshold.\n",
    "    \"\"\"\n",
    "    # Sort by score descending for cumulative computations\n",
    "    order = np.argsort(-scores)\n",
    "    y_true_sorted = y_true[order]\n",
    "    scores_sorted = scores[order]\n",
    "\n",
    "    # Total positives/negatives\n",
    "    P = (y_true == 1).sum()\n",
    "    N = (y_true == 0).sum()\n",
    "\n",
    "    # Sweep thresholds at each unique score\n",
    "    distinct_idxs = np.where(np.diff(scores_sorted, prepend=np.inf))[0]\n",
    "\n",
    "    TPR_list = []\n",
    "    FPR_list = []\n",
    "    thr_list = []\n",
    "\n",
    "    TP = FP = 0\n",
    "    # We iterate through sorted scores; at each index where score changes,\n",
    "    # we record the current (TPR, FPR) just *before* lowering threshold past that score.\n",
    "    for idx in range(len(scores_sorted)):\n",
    "        # Predict positive for everything up to idx\n",
    "        if y_true_sorted[idx] == 1:\n",
    "            TP += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "\n",
    "        # If next score is different\n",
    "        if (idx + 1 == len(scores_sorted)) or (scores_sorted[idx + 1] != scores_sorted[idx]):\n",
    "            TPR = TP / (P if P > 0 else 1)\n",
    "            FPR = FP / (N if N > 0 else 1)\n",
    "            TPR_list.append(TPR)\n",
    "            FPR_list.append(FPR)\n",
    "            thr_list.append(scores_sorted[idx])\n",
    "\n",
    "    # Add the (0,0) at threshold above max (predict none) and (1,1) at threshold below min (predict all)\n",
    "    # Ensure proper starting/ending anchors for integration\n",
    "    FPR_arr = np.array([0.0] + FPR_list + [1.0])\n",
    "    TPR_arr = np.array([0.0] + TPR_list + [1.0])\n",
    "    thr_arr = np.array([np.inf] + thr_list + [-np.inf])\n",
    "    return FPR_arr, TPR_arr, thr_arr\n",
    "\n",
    "FPR, TPR, THR = roc_curve_from_scores(y_true, scores)\n",
    "print(\"ROC points:\", len(FPR), \"FPR[0..3]=\", FPR[:3], \"TPR[0..3]=\", TPR[:3], \"THR[0..3]=\", THR[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmQ_HxN4Cybu"
   },
   "source": [
    "###  Compute AUC from Scratch  \n",
    "We calculate the **Area Under the ROC Curve (AUC)** using two methods:  \n",
    "1. **Trapezoidal rule (integration)**  \n",
    "2. **Pairwise ranking probability** interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2-0hPuKqCOdb",
    "outputId": "195e4a4e-6fe0-45d1-fa6f-d3cdf9616ab8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (trapezoid) = 0.8615\n",
      "AUC (pairwise ) = 0.8615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2734740610.py:11: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  return float(np.trapz(TPR, FPR))\n"
     ]
    }
   ],
   "source": [
    "# AUC (AUROC) via trapezoidal rule + pairwise interpretation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def auc_trapezoid(FPR: np.ndarray, TPR: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute area under ROC via trapezoidal rule.\n",
    "    Assumes FPR is monotonically increasing from 0 to 1.\n",
    "    \"\"\"\n",
    "    # np.trapz integrates TPR dFPR\n",
    "    return float(np.trapz(TPR, FPR))\n",
    "\n",
    "def auc_pairwise_probability(y_true: np.ndarray, scores: np.ndarray):\n",
    "    \"\"\"\n",
    "    Pairwise AUC interpretation: probability that a random positive has a higher score\n",
    "    than a random negative (ties count as 0.5).\n",
    "    \"\"\"\n",
    "    pos_scores = scores[y_true == 1]\n",
    "    neg_scores = scores[y_true == 0]\n",
    "    if len(pos_scores) == 0 or len(neg_scores) == 0:\n",
    "        return 0.5\n",
    "\n",
    "    # Efficient rank-based computation\n",
    "    # Concatenate and rank; AUC = (sum of positive ranks - m*(m+1)/2) / (m*n)\n",
    "    # where m=#pos, n=#neg; ranks are 1-based. Handle ties via average ranks.\n",
    "    all_scores = np.concatenate([pos_scores, neg_scores])\n",
    "    order = all_scores.argsort()\n",
    "    ranks = np.empty_like(order, dtype=float)\n",
    "    ranks[order] = np.arange(1, len(all_scores) + 1)\n",
    "\n",
    "    m = len(pos_scores)\n",
    "    n = len(neg_scores)\n",
    "    sum_pos_ranks = ranks[:m].sum()\n",
    "    auc = (sum_pos_ranks - m * (m + 1) / 2) / (m * n)\n",
    "    return float(auc)\n",
    "\n",
    "# Compute AUC both ways and compare\n",
    "auc_trap = auc_trapezoid(FPR, TPR)\n",
    "auc_pair = auc_pairwise_probability(y_true, scores)\n",
    "\n",
    "print(f\"AUC (trapezoid) = {auc_trap:.4f}\")\n",
    "print(f\"AUC (pairwise ) = {auc_pair:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cIDbtL9C2S5"
   },
   "source": [
    "### Find the Best F1 Threshold  \n",
    "We test multiple thresholds and find which one gives the highest **F1 score**,  \n",
    "helping us decide the best point for converting probabilities to binary labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r-x65n6wCP7j",
    "outputId": "46d6b64a-83de-4d2c-9c92-4f321bb6ff91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1=0.734 at threshold=0.418 (Precision=0.646, Recall=0.850)\n"
     ]
    }
   ],
   "source": [
    "# Find threshold that maximizes F1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def best_threshold_for_f1(y_true: np.ndarray, scores: np.ndarray):\n",
    "    \"\"\"\n",
    "    Search thresholds (unique score values + 0 and 1) to maximize F1.\n",
    "    Returns (best_threshold, best_F1, precision_at_best, recall_at_best).\n",
    "    \"\"\"\n",
    "    uniq = np.unique(scores)\n",
    "    candidates = np.concatenate([[0.0], uniq, [1.0]])\n",
    "    best = (-1.0, 0.0, 0.0, 0.0)\n",
    "\n",
    "    for thr in candidates:\n",
    "        TP, FP, TN, FN = confusion_counts_from_threshold(y_true, scores, thr)\n",
    "        P, R, F1 = precision_recall_f1_from_counts(TP, FP, TN, FN)\n",
    "        if F1 > best[1]:\n",
    "            best = (thr, F1, P, R)\n",
    "    return best\n",
    "\n",
    "thr, f1, P, R = best_threshold_for_f1(y_true, scores)\n",
    "print(f\"Best F1={f1:.3f} at threshold={thr:.3f} (Precision={P:.3f}, Recall={R:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRaYKs-aC6Em"
   },
   "source": [
    "### PyTorch Version of F1, ROC, and AUC  \n",
    "We re-implement the same logic using **PyTorch tensors**,  \n",
    "demonstrating how to compute F1, ROC, and AUC directly in a deep learning workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-DBofGxiCQ_L",
    "outputId": "99a70623-59e5-4265-af6f-f5708231e4ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Torch] Threshold=0.50 -> TP=41, FP=19, TN=121, FN=19\n",
      "[Torch] Precision=0.683, Recall=0.683, F1=0.683\n",
      "[Torch] AUC (trapz) = 0.8615\n"
     ]
    }
   ],
   "source": [
    "# Pure torch implementation (mirrors the NumPy logic)\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Convert our NumPy arrays to torch tensors\n",
    "y_true_t = torch.from_numpy(y_true).long()\n",
    "scores_t = torch.from_numpy(scores).float()\n",
    "\n",
    "def confusion_counts_torch(y_true_t: torch.Tensor, scores_t: torch.Tensor, threshold: float):\n",
    "    \"\"\"\n",
    "    Torch version: TP, FP, TN, FN at a given threshold.\n",
    "    \"\"\"\n",
    "    y_pred_t = (scores_t >= threshold).long()\n",
    "    TP = int(((y_pred_t == 1) & (y_true_t == 1)).sum().item())\n",
    "    FP = int(((y_pred_t == 1) & (y_true_t == 0)).sum().item())\n",
    "    TN = int(((y_pred_t == 0) & (y_true_t == 0)).sum().item())\n",
    "    FN = int(((y_pred_t == 0) & (y_true_t == 1)).sum().item())\n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "def precision_recall_f1_torch(TP: int, FP: int, TN: int, FN: int, eps: float = 1e-12):\n",
    "    TP = torch.tensor(TP, dtype=torch.float64)\n",
    "    FP = torch.tensor(FP, dtype=torch.float64)\n",
    "    TN = torch.tensor(TN, dtype=torch.float64)\n",
    "    FN = torch.tensor(FN, dtype=torch.float64)\n",
    "\n",
    "    precision = TP / (TP + FP + eps)\n",
    "    recall    = TP / (TP + FN + eps)\n",
    "    f1        = 2 * precision * recall / (precision + recall + eps)\n",
    "    return float(precision), float(recall), float(f1)\n",
    "\n",
    "def roc_curve_torch(y_true_t: torch.Tensor, scores_t: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute ROC (FPR, TPR, thresholds) using pure torch ops.\n",
    "    Returns CPU numpy arrays for convenience.\n",
    "    \"\"\"\n",
    "    # Sort by scores desc\n",
    "    scores_sorted, order = torch.sort(scores_t, descending=True)\n",
    "    y_sorted = y_true_t[order]\n",
    "\n",
    "    P = (y_true_t == 1).sum().item()\n",
    "    N = (y_true_t == 0).sum().item()\n",
    "    if P == 0 or N == 0:\n",
    "        # Degenerate case\n",
    "        FPR = torch.tensor([0.0, 1.0])\n",
    "        TPR = torch.tensor([0.0, 1.0])\n",
    "        THR = torch.tensor([float('inf'), float('-inf')])\n",
    "        return FPR.numpy(), TPR.numpy(), THR.numpy()\n",
    "\n",
    "    TPR_list = []\n",
    "    FPR_list = []\n",
    "    THR_list = []\n",
    "\n",
    "    TP = FP = 0\n",
    "    for i in range(len(scores_sorted)):\n",
    "        if y_sorted[i] == 1:\n",
    "            TP += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "        # Check threshold change\n",
    "        if (i + 1 == len(scores_sorted)) or (scores_sorted[i + 1] != scores_sorted[i]):\n",
    "            TPR_list.append(TP / P)\n",
    "            FPR_list.append(FP / N)\n",
    "            THR_list.append(float(scores_sorted[i].item()))\n",
    "\n",
    "    # Add anchors\n",
    "    FPR = torch.tensor([0.0] + FPR_list + [1.0], dtype=torch.float64)\n",
    "    TPR = torch.tensor([0.0] + TPR_list + [1.0], dtype=torch.float64)\n",
    "    THR = torch.tensor([float('inf')] + THR_list + [float('-inf')], dtype=torch.float64)\n",
    "\n",
    "    return FPR.numpy(), TPR.numpy(), THR.numpy()\n",
    "\n",
    "def auc_trapz_torch(FPR_np, TPR_np):\n",
    "    \"\"\"\n",
    "    Torch trapezoidal AUC (accepts numpy arrays for convenience).\n",
    "    \"\"\"\n",
    "    FPR = torch.from_numpy(FPR_np).double()\n",
    "    TPR = torch.from_numpy(TPR_np).double()\n",
    "    # Integrate TPR dFPR\n",
    "    auc = torch.trapz(TPR, FPR)\n",
    "    return float(auc.item())\n",
    "\n",
    "# Demo: F1 at threshold 0.5\n",
    "TP, FP, TN, FN = confusion_counts_torch(y_true_t, scores_t, threshold=0.5)\n",
    "P, R, F1 = precision_recall_f1_torch(TP, FP, TN, FN)\n",
    "print(f\"[Torch] Threshold=0.50 -> TP={TP}, FP={FP}, TN={TN}, FN={FN}\")\n",
    "print(f\"[Torch] Precision={P:.3f}, Recall={R:.3f}, F1={F1:.3f}\")\n",
    "\n",
    "# Demo: ROC + AUC\n",
    "FPR_np, TPR_np, THR_np = roc_curve_torch(y_true_t, scores_t)\n",
    "auc_t = auc_trapz_torch(FPR_np, TPR_np)\n",
    "print(f\"[Torch] AUC (trapz) = {auc_t:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FPkgiOHC8XZ"
   },
   "source": [
    "###  Find Best F1 Threshold (PyTorch Version)  \n",
    "We search for the threshold that maximizes F1 using PyTorch and NumPy together.  \n",
    "This step helps us verify consistency between NumPy and PyTorch implementations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CI-jLPFBCSAx",
    "outputId": "bbd0713c-9f4c-434f-acdf-f2241b49acaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Torch] Best F1=0.734 at threshold=0.418 (Precision=0.646, Recall=0.850)\n"
     ]
    }
   ],
   "source": [
    "# choose the threshold that maximizes F1 (pure torch/np hybrid)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def best_threshold_for_f1_torch(y_true_t: torch.Tensor, scores_t: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Evaluate F1 on a grid of candidate thresholds (unique scores + [0,1]).\n",
    "    Returns best (threshold, F1, precision, recall).\n",
    "    \"\"\"\n",
    "    uniq = torch.unique(scores_t).cpu().numpy()\n",
    "    candidates = np.concatenate([[0.0], uniq, [1.0]])\n",
    "    best = (-1.0, 0.0, 0.0, 0.0)\n",
    "\n",
    "    y_true_np = y_true_t.cpu().numpy()\n",
    "    scores_np = scores_t.cpu().numpy()\n",
    "\n",
    "    for thr in candidates:\n",
    "        y_pred = (scores_np >= thr).astype(int)\n",
    "        TP = int(((y_pred == 1) & (y_true_np == 1)).sum())\n",
    "        FP = int(((y_pred == 1) & (y_true_np == 0)).sum())\n",
    "        TN = int(((y_pred == 0) & (y_true_np == 0)).sum())\n",
    "        FN = int(((y_pred == 0) & (y_true_np == 1)).sum())\n",
    "        P = TP / (TP + FP + 1e-12)\n",
    "        R = TP / (TP + FN + 1e-12)\n",
    "        F1 = 2 * P * R / (P + R + 1e-12)\n",
    "        if F1 > best[1]:\n",
    "            best = (float(thr), float(F1), float(P), float(R))\n",
    "    return best\n",
    "\n",
    "thr_t, f1_t, P_t, R_t = best_threshold_for_f1_torch(y_true_t, scores_t)\n",
    "print(f\"[Torch] Best F1={f1_t:.3f} at threshold={thr_t:.3f} (Precision={P_t:.3f}, Recall={R_t:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6Vzj2P2ACTta"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
