{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "174b61cb-1d87-4f98-b967-6383cdf37bbf",
   "metadata": {},
   "source": [
    "# Understanding Accuracy, Precision & Recall\n",
    "\n",
    "Evaluating a classification model means going beyond accuracy â€” each metric tells a **different story** about model performance.  \n",
    "Hereâ€™s a clear breakdown of **when to use each** and **what they mean** in real-world settings.\n",
    "\n",
    "---\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "**Definition:**  \n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "**Meaning:**  \n",
    "Measures the overall correctness â€” how often the model is right.\n",
    "\n",
    "**Use When:**\n",
    "- Classes are **balanced**.\n",
    "- All types of mistakes are **equally costly**.\n",
    "\n",
    "**Avoid When:**\n",
    "- Dataset is **imbalanced** (e.g., 99% negatives).\n",
    "\n",
    "**Examples:**\n",
    "- Handwritten digit recognition (MNIST)  \n",
    "- Image classification with balanced classes  \n",
    "- Language model next-word prediction  \n",
    "\n",
    "âš ï¸ **Example of misleading accuracy:**  \n",
    "If 99% of samples are negative, predicting â€œnegativeâ€ always gives 99% accuracy â€” even if the model never detects a true positive.\n",
    "\n",
    "---\n",
    "\n",
    "## Precision\n",
    "\n",
    "**Definition:**  \n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "**Meaning:**  \n",
    "When the model predicts **positive**, how often is it actually correct?  \n",
    "It measures **exactness** and **trustworthiness** of positive predictions.\n",
    "\n",
    "**Use When:**\n",
    "- **False Positives (FP)** are costly.\n",
    "- You need high confidence before labeling something as positive.\n",
    "\n",
    "**Avoid When:**\n",
    "- You can tolerate some false alarms but missing positives is worse.\n",
    "\n",
    "**Examples:**\n",
    "- Spam filter (donâ€™t mark real emails as spam)  \n",
    "- Credit card fraud detection (avoid wrongly blocking real transactions)  \n",
    "- Cancer detection model (avoid false diagnoses)  \n",
    "\n",
    "ðŸ’¡ *High Precision â†’ â€œWhen I say something is positive, Iâ€™m almost always right.â€*\n",
    "\n",
    "---\n",
    "\n",
    "## Recall\n",
    "\n",
    "**Definition:**  \n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "**Meaning:**  \n",
    "Out of all **actual positives**, how many did the model catch?  \n",
    "It measures **sensitivity** or **completeness**.\n",
    "\n",
    "**Use When:**\n",
    "- **False Negatives (FN)** are costly.\n",
    "- You want to detect **all** positives, even if you get some false alarms.\n",
    "\n",
    "**Avoid When:**\n",
    "- You want to be very selective or confident before predicting positives.\n",
    "\n",
    "**Examples:**\n",
    "- Disease detection (donâ€™t miss sick patients)  \n",
    "- Security / anomaly detection (better to over-alert than miss an attack)  \n",
    "- Search engines (retrieve all relevant documents)  \n",
    "\n",
    "*High Recall â†’ â€œI catch most of the true positives, even if I sometimes make mistakes.â€*\n",
    "\n",
    "---\n",
    "\n",
    "## Precisionâ€“Recall Trade-Off\n",
    "\n",
    "- Increasing **recall** (catching more positives) usually lowers **precision** (more false alarms).  \n",
    "- Increasing **precision** (being more selective) usually lowers **recall** (misses some positives).\n",
    "\n",
    "To balance both, we often use the **F1 Score**:\n",
    "\n",
    "$$\n",
    "\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision + Recall}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Metric | Measures | Best Used When | Avoid When | Example |\n",
    "|:--|:--|:--|:--|:--|\n",
    "| **Accuracy** | Overall correctness | Classes are balanced | Dataset is imbalanced | Digit recognition |\n",
    "| **Precision** | Reliability of positive predictions | False positives are costly | Missing positives is acceptable | Spam / Fraud detection |\n",
    "| **Recall** | Ability to detect all positives | False negatives are costly | You need selectivity | Disease / Security detection |\n",
    "\n",
    "---\n",
    "\n",
    "**Key takeaway:**  \n",
    "- Use **Accuracy** when your dataset is balanced.  \n",
    "- Use **Precision** when you must be *right* about positives.  \n",
    "- Use **Recall** when you must *catch* all positives.  \n",
    "- Use **F1 Score** when you want a **balanced view** between the two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0ceb8f2-21fa-4b50-b896-35a00bf94e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# True labels (ground truth)\n",
    "y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\n",
    "\n",
    "# Predicted labels (model outputs)\n",
    "y_pred = np.array([1, 0, 1, 0, 0, 1, 0, 1, 1, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37426bc2-42ae-4efa-8c6c-bf1c0ed49039",
   "metadata": {},
   "source": [
    "We define two arrays:\n",
    "- `y_true`: actual class labels (0 = negative, 1 = positive)  \n",
    "- `y_pred`: model predictions\n",
    "\n",
    "Next, weâ€™ll build a **confusion matrix** to see TP, TN, FP, FN counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51020213-dad1-4a54-b8e4-40fb008ac560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 1],\n",
       "       [1, 4]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69626c8-f244-463a-9770-61bd5f8640ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "776ba4ff-5178-4c0d-8715-f74b343fff1b",
   "metadata": {},
   "source": [
    "The **confusion matrix** shows:\n",
    "\n",
    "|               | Predicted 0 | Predicted 1 |\n",
    "|---------------|-------------|-------------|\n",
    "| **Actual 0**  | TN          | FP          |\n",
    "| **Actual 1**  | FN          | TP          |\n",
    "\n",
    "From this matrix we can compute metrics:\n",
    "\n",
    "- **TP** = true positives â†’ correct positive predictions  \n",
    "- **TN** = true negatives â†’ correct negative predictions  \n",
    "- **FP** = false positives â†’ wrongly predicted as positive  \n",
    "- **FN** = false negatives â†’ missed positives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6568ab21-b906-42c7-878a-0239112ebbb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.8), np.float64(0.8), np.float64(0.8))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "accuracy, precision, recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e584882-7860-4883-9b32-59395bab9837",
   "metadata": {},
   "source": [
    "We compute each metric manually:\n",
    "\n",
    "- **Accuracy** = (TP + TN) / Total  \n",
    "- **Precision** = TP / (TP + FP)  \n",
    "- **Recall** = TP / (TP + FN)\n",
    "\n",
    "These values give insight into the modelâ€™s performance from different angles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e0e8ef7-dffa-4eda-bd9a-dfd32395cbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8\n",
      "Precision: 0.8\n",
      "Recall   : 0.8\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
    "print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "print(\"Recall   :\", recall_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50a630d-9f9c-411d-afbd-c8e32d20000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbf37fa6-352c-48d6-8f32-03315947e6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchMetrics Accuracy : 0.800000011920929\n",
      "TorchMetrics Precision: 0.800000011920929\n",
      "TorchMetrics Recall   : 0.800000011920929\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall\n",
    "\n",
    "\n",
    "# Dummy binary classification outputs\n",
    "y_true = torch.tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\n",
    "y_pred = torch.tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0])\n",
    "\n",
    "\n",
    "acc = BinaryAccuracy()\n",
    "prec = BinaryPrecision()\n",
    "rec = BinaryRecall()\n",
    "\n",
    "print(\"TorchMetrics Accuracy :\", acc(y_pred, y_true).item())\n",
    "print(\"TorchMetrics Precision:\", prec(y_pred, y_true).item())\n",
    "print(\"TorchMetrics Recall   :\", rec(y_pred, y_true).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cff7af-4801-4a26-9cc4-88a1daa1dc0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9285f3fd-cf1e-45b7-a54f-8a0ba0d73202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives in data: 9 / 100\n",
      "\n",
      "Threshold = 0.50\n",
      "Confusion Matrix -> TP:9  FP:0  FN:0  TN:91\n",
      "Accuracy : 1.000\n",
      "Precision: 1.000\n",
      "Recall   : 1.000\n",
      "\n",
      "Threshold = 0.30\n",
      "Confusion Matrix -> TP:9  FP:9  FN:0  TN:82\n",
      "Accuracy : 0.910\n",
      "Precision: 0.500\n",
      "Recall   : 1.000\n"
     ]
    }
   ],
   "source": [
    "# Example: medical screening for a rare disease (positives are rare)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Create imbalanced dummy data (10% positives)\n",
    "n = 100\n",
    "y_true = (torch.rand(n) < 0.10).long() # 1 = has disease, 0 = healthy (rare positives)\n",
    "\n",
    "# positives tend to have higher scores than negatives, but with overlap\n",
    "y_scores = 0.65 * y_true + 0.35 * torch.rand(n)\n",
    "\n",
    "# Helper to compute confusion & metrics for a given threshold \n",
    "def metrics_at_threshold(scores, labels, thresh=0.50):\n",
    "    y_pred = (scores >= thresh).long()\n",
    "\n",
    "    TP = ((labels == 1) & (y_pred == 1)).sum().item()\n",
    "    TN = ((labels == 0) & (y_pred == 0)).sum().item()\n",
    "    FP = ((labels == 0) & (y_pred == 1)).sum().item()\n",
    "    FN = ((labels == 1) & (y_pred == 0)).sum().item()\n",
    "\n",
    "    total = TP + TN + FP + FN\n",
    "    acc = (TP + TN) / total if total else 0.0\n",
    "    prec = TP / (TP + FP) if (TP + FP) else 0.0\n",
    "    rec = TP / (TP + FN) if (TP + FN) else 0.0\n",
    "\n",
    "    return {\n",
    "        \"threshold\": thresh,\n",
    "        \"TP\": TP, \"FP\": FP, \"FN\": FN, \"TN\": TN,\n",
    "        \"accuracy\": acc, \"precision\": prec, \"recall\": rec\n",
    "    }\n",
    "\n",
    "# Compare two thresholds: conservative vs. sensitive screening\n",
    "results = [\n",
    "    metrics_at_threshold(y_scores, y_true, thresh=0.50),  # more conservative (higher precision, lower recall)\n",
    "    metrics_at_threshold(y_scores, y_true, thresh=0.30),  # more sensitive (higher recall, lower precision)\n",
    "]\n",
    "\n",
    "\n",
    "print(f\"Positives in data: {y_true.sum().item()} / {n}\")\n",
    "for r in results:\n",
    "    print(f\"\\nThreshold = {r['threshold']:.2f}\")\n",
    "    print(f\"Confusion Matrix -> TP:{r['TP']}  FP:{r['FP']}  FN:{r['FN']}  TN:{r['TN']}\")\n",
    "    print(f\"Accuracy : {r['accuracy']:.3f}\")\n",
    "    print(f\"Precision: {r['precision']:.3f}\")\n",
    "    print(f\"Recall   : {r['recall']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa8b86d-737d-42fc-b37a-e5f5b06cc921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c4484a-99b6-4ee7-b201-62ce041b1ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f768ad-9e65-4ee3-87b9-3b2ff11976b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170f87f1-2dcf-432f-8033-d3ef1e9bb025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-dev)",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
