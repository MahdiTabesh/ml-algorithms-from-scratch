{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e62bf33a-e0a2-4a60-b8f0-b21f59c6ac44",
   "metadata": {},
   "source": [
    "# Dropout (Forward + Backward)\n",
    "\n",
    "**Concept:**  \n",
    "Dropout is a regularization technique to reduce overfitting in neural networks by randomly \"dropping\" units during training.\n",
    "\n",
    "- During **training**, each neuron is kept with probability $1-p$ and dropped (set to zero) with probability $p$.  \n",
    "- To maintain the expected scale of activations, the outputs of kept neurons are scaled by $\\frac{1}{1-p}$.  \n",
    "- During **inference**, dropout is disabled (all neurons active) since the scaling has already been accounted for.\n",
    "\n",
    "---\n",
    "\n",
    "**Formulas:**\n",
    "\n",
    "- **Forward pass (training):**\n",
    "  $$\n",
    "  \\tilde{h} = \\frac{m \\odot h}{1-p}, \\quad m \\sim \\text{Bernoulli}(1-p)\n",
    "  $$\n",
    "\n",
    "- **Backward pass:**\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial h} = \\frac{m \\odot \\frac{\\partial L}{\\partial \\tilde{h}}}{1-p}\n",
    "  $$\n",
    "\n",
    "where:\n",
    "- $ h $: input activations  \n",
    "- $ \\tilde{h} $: output after dropout  \n",
    "- $ m $: dropout mask (0/1 values)  \n",
    "- $ p $: dropout probability  \n",
    "\n",
    "---\n",
    "\n",
    "**Key Points:**\n",
    "- Dropout reduces overfitting by preventing co-adaptation of neurons.  \n",
    "- Different masks are applied independently at each training step.  \n",
    "- Backpropagation uses the same mask to ensure consistent gradient flow.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a419daf-c0f4-41a2-9aac-29e73d0f19a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87f33fd9-0c3b-4eda-95c0-aef472aa983a",
   "metadata": {},
   "source": [
    "# Dropout (Forward + Backward) — From Scratch\n",
    "\n",
    "**Idea.** During **training**, randomly drop each activation with probability \\(p\\) (set to 0) and scale the survivors by $\\tfrac{1}{1-p}$ so the expected activation stays unchanged. During **inference**, disable dropout.\n",
    "\n",
    "**Forward (training)**\n",
    "$$\n",
    "\\tilde{h} \\;=\\; \\frac{m \\odot h}{1-p}, \\qquad m \\sim \\mathrm{Bernoulli}(1-p)\n",
    "$$\n",
    "\n",
    "**Backward**\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h} \\;=\\; \\frac{m \\odot \\frac{\\partial L}{\\partial \\tilde{h}}}{1-p}\n",
    "$$\n",
    "\n",
    "We’ll insert dropout **after ReLU** on the hidden layer of a 2-layer MLP and implement the gradients manually (no autograd).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c78907c-e61d-4a34-bb99-345b3339741b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[manual+dropout] epoch  50 | loss 1.0639 | acc 0.594\n",
      "[manual+dropout] epoch 100 | loss 0.7131 | acc 0.758\n",
      "[manual+dropout] epoch 150 | loss 0.4612 | acc 0.844\n",
      "[manual+dropout] epoch 200 | loss 0.2999 | acc 0.922\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Toy data\n",
    "N, d, H, K = 128, 20, 64, 5\n",
    "X = torch.randn(N, d)\n",
    "y = torch.randint(0, K, (N,))\n",
    "\n",
    "# One-hot labels\n",
    "Y = torch.zeros(N, K)\n",
    "Y[torch.arange(N), y] = 1.0\n",
    "\n",
    "# Parameters\n",
    "W1 = torch.randn(d, H) * 0.02\n",
    "b1 = torch.zeros(H)\n",
    "W2 = torch.randn(H, K) * 0.02\n",
    "b2 = torch.zeros(K)\n",
    "\n",
    "# Hyperparams\n",
    "lr = 0.5\n",
    "epochs = 200\n",
    "p_drop = 0.5 # dropout probability\n",
    "training = True\n",
    "\n",
    "def softmax(logits):\n",
    "    z = logits - logits.max(dim=1, keepdim=True).values\n",
    "    expz = torch.exp(z)\n",
    "    return expz / expz.sum(dim=1, keepdim=True)\n",
    "\n",
    "def dropout_forward(h, p, training):\n",
    "    \"\"\"\n",
    "    Returns (h_tilde, mask).\n",
    "    If not training, returns (h, None).\n",
    "    \"\"\"\n",
    "    if not training or p == 0.0:\n",
    "        return h, None\n",
    "    keep_prob = 1.0 - p\n",
    "    mask = (torch.rand_like(h) < keep_prob).float()\n",
    "    h_tilde = (mask * h) / keep_prob\n",
    "    return h_tilde, mask\n",
    "\n",
    "def dropout_backward(grad_out, mask, p):\n",
    "    \"\"\"\n",
    "    Backprop through dropout using the SAME mask from forward.\n",
    "    \"\"\"\n",
    "    if mask is None or p == 0.0:\n",
    "        return grad_out\n",
    "    keep_prob = 1.0 - p\n",
    "    return (mask * grad_out) / keep_prob\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward\n",
    "    Z1 = X @ W1 + b1\n",
    "    H1 = torch.clamp(Z1, min=0.0)\n",
    "    H1_do, mask = dropout_forward(H1, p_drop, training)  # Dropout after ReLU\n",
    "    S  = H1_do @ W2 + b2\n",
    "    P  = softmax(S)\n",
    "    loss = -(Y * (P + 1e-12).log()).sum() / N\n",
    "\n",
    "    # Backward (manual)\n",
    "    dS  = (P - Y) / N\n",
    "    dW2 = H1_do.t() @ dS\n",
    "    db2 = dS.sum(dim=0)\n",
    "\n",
    "    dH1_do = dS @ W2.t()\n",
    "    dH1 = dropout_backward(dH1_do, mask, p_drop)        # back through dropout\n",
    "    dZ1 = dH1 * (Z1 > 0).float()\n",
    "\n",
    "    dW1 = X.t() @ dZ1\n",
    "    db1 = dZ1.sum(dim=0)\n",
    "\n",
    "    # SGD step\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        pred = P.argmax(dim=1)\n",
    "        acc = (pred == y).float().mean().item()\n",
    "        print(f\"[manual+dropout] epoch {epoch+1:3d} | loss {loss.item():.4f} | acc {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd534c5-1486-47aa-ab3c-f3657f109c43",
   "metadata": {},
   "source": [
    "# Dropout with PyTorch `nn.Dropout` (Autograd)\n",
    "\n",
    "Here we use PyTorch modules so autograd handles gradients.\n",
    "\n",
    "**Model (train mode):**\n",
    "$$\n",
    "\\text{Linear}(d\\!\\to\\!H) \\;\\rightarrow\\; \\mathrm{ReLU} \\;\\rightarrow\\; \\mathrm{Dropout}(p) \\;\\rightarrow\\; \\text{Linear}(H\\!\\to\\!K)\n",
    "$$\n",
    "\n",
    "- `model.train()` enables dropout (applies the Bernoulli mask and $\\tfrac{1}{1-p}$ scaling).\n",
    "- `model.eval()` disables dropout for inference.\n",
    "- Loss: `nn.CrossEntropyLoss` (log-softmax + NLL internally).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c4ed427-7ef8-4949-ab34-370347078eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nn.Dropout]    epoch  50 | loss 0.8975 | acc 0.680\n",
      "[nn.Dropout]    epoch 100 | loss 0.5385 | acc 0.836\n",
      "[nn.Dropout]    epoch 150 | loss 0.3482 | acc 0.891\n",
      "[nn.Dropout]    epoch 200 | loss 0.2693 | acc 0.922\n",
      "[eval] accuracy (dropout off): 1.000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Same toy data\n",
    "N, d, H, K = 128, 20, 64, 5\n",
    "X = torch.randn(N, d)\n",
    "y = torch.randint(0, K, (N,))\n",
    "\n",
    "p_drop = 0.5\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(d, H),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p_drop),   # dropout placed after ReLU\n",
    "    nn.Linear(H, K)\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "model.train()  # enable dropout\n",
    "for epoch in range(200):\n",
    "    optim.zero_grad()\n",
    "    logits = model(X)             # dropout active here\n",
    "    loss = criterion(logits, y)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        acc = (logits.argmax(1) == y).float().mean().item()\n",
    "        print(f\"[nn.Dropout]    epoch {epoch+1:3d} | loss {loss.item():.4f} | acc {acc:.3f}\")\n",
    "\n",
    "# Inference (disable dropout):\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X)             # dropout OFF\n",
    "    acc = (logits.argmax(1) == y).float().mean().item()\n",
    "    print(f\"[eval] accuracy (dropout off): {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960be1a-a187-4a0d-b15d-4d83d82ae01d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0888fe8-1167-4917-9bd0-b11406d51e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-dev)",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
