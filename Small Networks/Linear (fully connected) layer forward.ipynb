{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbbc166d-019d-4fea-afd5-f928ef638fcc",
   "metadata": {},
   "source": [
    "# Linear (Fully Connected) Layer — Forward Pass\n",
    "\n",
    "**Goal:** Implement the forward pass of a linear (fully connected) layer.  \n",
    "This layer applies an **affine** transformation to inputs.\n",
    "\n",
    "---\n",
    "\n",
    "## Definitions & Shapes\n",
    "\n",
    "- Batch input **X** shape: `(N, d_in)`\n",
    "- Weights **W** shape: `(d_in, d_out)`\n",
    "- Bias **b** shape: `(d_out,)`\n",
    "- Output **Y** shape: `(N, d_out)`\n",
    "\n",
    "---\n",
    "\n",
    "## Formulas\n",
    "\n",
    "**Single example** (vector x):\n",
    "\n",
    "$$\n",
    "y = W^{\\top} x + b\n",
    "$$\n",
    "\n",
    "**Batch form** (matrix X):\n",
    "\n",
    "$$\n",
    "Y = XW + \\mathbf{1}\\, b^{\\top} \\;\\equiv\\; Y = XW + b\n",
    "$$\n",
    "\n",
    "Here, $ \\mathbf{1} $ is an all-ones column; in code the bias adds by **broadcasting**.\n",
    "\n",
    "**PyTorch convention** (`nn.Linear(in_features, out_features)`) stores $ \\tilde W $ as `(d_out, d_in)` and computes:\n",
    "\n",
    "$$\n",
    "Y = X\\, \\tilde W^{\\top} + b\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition\n",
    "\n",
    "Each output dimension \\( j \\) is a weighted sum of inputs plus a bias:\n",
    "\n",
    "$$\n",
    "Y_{:,j} = X \\, W_{:,j} + b_j\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Complexity\n",
    "\n",
    "- Time: $ \\mathcal{O}(N \\cdot d_{\\text{in}} \\cdot d_{\\text{out}}) $ \n",
    "- Params: $ d_{\\text{in}} \\cdot d_{\\text{out}} + d_{\\text{out}} $\n",
    "\n",
    "---\n",
    "\n",
    "## Shape Checklist\n",
    "\n",
    "- `X: (N, d_in)`\n",
    "- `W: (d_in, d_out)` → use `X @ W`\n",
    "- `b: (d_out,)` → broadcasts over batch\n",
    "- `Y: (N, d_out)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "636274bc-b20c-4fc8-a098-79d02b8e79bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a058422-72c6-48a5-8519-e42239aaebbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2febde46-479d-4f2b-a6fb-9b1c847969fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(X, weight, bias=None):\n",
    "    \"\"\"\n",
    "    X:       (N, d_in)\n",
    "    weight:  (d_out, d_in)    # same as nn.Linear\n",
    "    bias:    (d_out,) or None\n",
    "    returns: (N, d_out)\n",
    "    \"\"\"\n",
    "    Y = X @ weight.t()\n",
    "    if bias is not None:\n",
    "        Y = Y + bias\n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd89caa2-e654-4632-bdb4-4377b46cbe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3]) torch.Size([4, 3]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "X = torch.randn(5, 3)\n",
    "W = torch.randn(4, 3)\n",
    "b = torch.randn(4)\n",
    "\n",
    "# Gradients flow\n",
    "X = torch.randn(5, 3, requires_grad=True)\n",
    "W = torch.randn(4, 3, requires_grad=True)\n",
    "b = torch.randn(4,     requires_grad=True)\n",
    "loss = linear_forward(X, W, b).pow(2).mean()\n",
    "loss.backward()\n",
    "print(X.grad.shape, W.grad.shape, b.grad.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94167826-b5c0-4171-bef5-a0a4bd10fabf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30e0946b-a202-4a8f-b026-95ce46c5aa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    A manual Linear layer:\n",
    "      X: (N, d_in)\n",
    "      weight: (d_out, d_in)\n",
    "      bias: (d_out,)\n",
    "      forward: Y = X @ weight.T + bias\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Parameters\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = X.matmul(self.weight.T)\n",
    "        if self.bias is not None:\n",
    "            Y = Y + self.bias\n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9eb4584-e69d-47a7-9a02-8609abef5cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# random batch\n",
    "torch.manual_seed(0)\n",
    "X = torch.randn(4, 3)\n",
    "\n",
    "# reference layer\n",
    "ref = nn.Linear(3, 2, bias=True)\n",
    "\n",
    "# our layer with same params\n",
    "mine = MyLinear(3, 2, bias=True)\n",
    "with torch.no_grad():\n",
    "    mine.weight.copy_(ref.weight) \n",
    "    mine.bias.copy_(ref.bias)\n",
    "\n",
    "# outputs should match\n",
    "Y_ref  = ref(X)\n",
    "Y_mine = mine(X)\n",
    "print(torch.allclose(Y_ref, Y_mine, atol=1e-6)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6540ebc-62fc-462c-b27b-14632ba35d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss: 0.17782405018806458\n"
     ]
    }
   ],
   "source": [
    "target = torch.randn(4, 2)\n",
    "opt = torch.optim.SGD(mine.parameters(), lr=0.1)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for _ in range(50):\n",
    "    opt.zero_grad()\n",
    "    loss = loss_fn(mine(X), target)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "print(\"final loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01fb21-e25b-4499-8aa2-7b7d0404bedd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b9e7c91-8356-4a8f-bcfa-cd372e2f7051",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_in, hidden, d_out):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, d_out),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b27a7019-8d11-45b8-b3be-cb4d12df65eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2854,  0.2482],\n",
       "        [-0.3773,  0.0953],\n",
       "        [-0.3697, -0.1341],\n",
       "        [-0.4097,  0.1283],\n",
       "        [-0.4590,  0.0427]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(d_in=3, hidden=8, d_out=2)\n",
    "out = model(torch.randn(5, 3))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f052bce9-9231-4c4a-8113-347c92a2339a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-dev)",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
