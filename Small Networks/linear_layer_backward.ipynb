{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "822f9b37-e8bc-47e5-98e5-7bf94308cc0c",
   "metadata": {},
   "source": [
    "# Linear Layer Backward Pass (Gradients)\n",
    "\n",
    "The linear (fully connected) layer computes:\n",
    "\n",
    "$$\n",
    "y = xW^T + b\n",
    "$$\n",
    "\n",
    "- $x \\in \\mathbb{R}^{N \\times d_{in}}$ (input batch)  \n",
    "- $W \\in \\mathbb{R}^{d_{out} \\times d_{in}}$ (weights)  \n",
    "- $b \\in \\mathbb{R}^{d_{out}}$ (bias)  \n",
    "- $y \\in \\mathbb{R}^{N \\times d_{out}}$ (output)  \n",
    "\n",
    "---\n",
    "\n",
    "### Backward Derivatives:\n",
    "\n",
    "1. Gradient w.r.t input:\n",
    "$$\n",
    "dX = dY \\cdot W\n",
    "$$\n",
    "\n",
    "2. Gradient w.r.t weights:\n",
    "$$\n",
    "dW = dY^T \\cdot x\n",
    "$$\n",
    "\n",
    "3. Gradient w.r.t bias:\n",
    "$$\n",
    "db = \\sum_{i=1}^N dY_i\n",
    "$$\n",
    "\n",
    "This ensures gradients flow backward through the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b065c715-ebcb-42e2-b246-abc5b99aa0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dX shape: (4, 3)\n",
      "dW shape: (2, 3)\n",
      "db shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dummy data\n",
    "N, d_in, d_out = 4, 3, 2\n",
    "x = np.random.randn(N, d_in)\n",
    "W = np.random.randn(d_out, d_in)\n",
    "b = np.random.randn(d_out)\n",
    "\n",
    "# Forward pass\n",
    "y = x @ W.T + b\n",
    "\n",
    "# Assume upstream gradient (from next layer / loss)\n",
    "dY = np.random.randn(N, d_out)\n",
    "\n",
    "# Backward pass\n",
    "dX = dY @ W\n",
    "dW = dY.T @ x \n",
    "db = dY.sum(axis=0) \n",
    "\n",
    "print(\"dX shape:\", dX.shape)\n",
    "print(\"dW shape:\", dW.shape)\n",
    "print(\"db shape:\", db.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc45d25-6d85-481c-851f-7952abe99494",
   "metadata": {},
   "source": [
    "# Verifying with PyTorch Autograd\n",
    "We can confirm our manual gradients using PyTorchâ€™s `autograd`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d77d7c-88a5-4786-85a6-91058ce912d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch dX: torch.Size([4, 3])\n",
      "PyTorch dW: torch.Size([2, 3])\n",
      "PyTorch db: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Dummy data\n",
    "x = torch.randn(N, d_in, requires_grad=True)\n",
    "W = torch.randn(d_out, d_in, requires_grad=True)\n",
    "b = torch.randn(d_out, requires_grad=True)\n",
    "\n",
    "# Forward\n",
    "y = x @ W.T + b\n",
    "\n",
    "# Dummy loss (sum of outputs)\n",
    "loss = y.sum()\n",
    "\n",
    "# Backward\n",
    "loss.backward()\n",
    "\n",
    "print(\"PyTorch dX:\", x.grad.shape)\n",
    "print(\"PyTorch dW:\", W.grad.shape)\n",
    "print(\"PyTorch db:\", b.grad.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfae18fd-3857-486d-a455-674365610342",
   "metadata": {},
   "source": [
    "# Linear Layer + MSE Loss: Forward & Backward\n",
    "\n",
    "We use a linear (fully connected) layer:\n",
    "$$\n",
    "y = x W^{\\top} + b\n",
    "$$\n",
    "- $x \\in \\mathbb{R}^{N \\times d_{in}},\\;\\; W \\in \\mathbb{R}^{d_{out} \\times d_{in}},\\;\\; b \\in \\mathbb{R}^{d_{out}},\\;\\; y \\in \\mathbb{R}^{N \\times d_{out}}$.\n",
    "\n",
    "We choose **mean squared error (MSE)** against targets $t \\in \\mathbb{R}^{N \\times d_{out}}$:\n",
    "$$\n",
    "L = \\frac{1}{2N}\\sum_{i=1}^{N}\\lVert y_i - t_i \\rVert_2^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Backward (using chain rule)\n",
    "\n",
    "First, the loss gradient w.r.t. layer output:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial y} = dY = \\frac{1}{N}(y - t)\n",
    "$$\n",
    "\n",
    "Then the linear layer gradients:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX &= dY \\cdot W \\\\\n",
    "dW &= dY^{\\top} \\cdot x \\\\\n",
    "db &= \\sum_{i=1}^{N} dY_{i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $dX=\\frac{\\partial L}{\\partial x}$ is needed to pass gradients to earlier layers.  \n",
    "- $dW=\\frac{\\partial L}{\\partial W}$ and $db=\\frac{\\partial L}{\\partial b}$ are used to update parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d332b97b-adcf-4014-9efa-7961ff54da95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.859075\n",
      "dX: (5, 4)  dW: (3, 4)  db: (3,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Reproducibility\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "#dummy data\n",
    "N, d_in, d_out = 5, 4, 3\n",
    "x = rng.normal(size=(N, d_in))\n",
    "t = rng.normal(size=(N, d_out))\n",
    "\n",
    "# Parameters\n",
    "W = rng.normal(size=(d_out, d_in))\n",
    "b = rng.normal(size=(d_out,))\n",
    "\n",
    "# Forward \n",
    "y = x @ W.T + b \n",
    "\n",
    "# MSE loss\n",
    "residual = y - t\n",
    "L = 0.5 / N * np.sum(residual**2)\n",
    "\n",
    "# Backward\n",
    "dY = (1.0 / N) * residual\n",
    "dX = dY @ W\n",
    "dW = dY.T @ x\n",
    "db = dY.sum(axis=0)\n",
    "\n",
    "print(f\"Loss: {L:.6f}\")\n",
    "print(\"dX:\", dX.shape, \" dW:\", dW.shape, \" db:\", db.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d9988f8-9ccc-4f61-8aab-0b16c698acc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss torch: 7.859075\n",
      "Match dW: True | Match db: True | Match dX: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Convert NumPy -> Torch with the same values\n",
    "xt = torch.tensor(x, dtype=torch.float32, requires_grad=True)\n",
    "Wt = torch.tensor(W, dtype=torch.float32, requires_grad=True)\n",
    "bt = torch.tensor(b, dtype=torch.float32, requires_grad=True)\n",
    "tt = torch.tensor(t, dtype=torch.float32)\n",
    "\n",
    "# Forward\n",
    "yt = xt @ Wt.T + bt\n",
    "Lt = 0.5 / N * torch.sum((yt - tt)**2)\n",
    "\n",
    "# Backward\n",
    "Lt.backward()\n",
    "\n",
    "# Compare (allow small numerical diffs)\n",
    "np_allclose = lambda a,b: np.allclose(a, b, atol=1e-6, rtol=1e-5)\n",
    "\n",
    "ok_W = np_allclose(Wt.grad.detach().numpy(), dW)\n",
    "ok_b = np_allclose(bt.grad.detach().numpy(), db)\n",
    "ok_X = np_allclose(xt.grad.detach().numpy(), dX)\n",
    "\n",
    "print(f\"Loss torch: {Lt.item():.6f}\")\n",
    "print(\"Match dW:\", ok_W, \"| Match db:\", ok_b, \"| Match dX:\", ok_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7480f182-d71f-4880-9589-3bf7d00cd236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-dev)",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
