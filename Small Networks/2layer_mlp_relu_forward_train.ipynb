{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4185b3fe-564a-457b-98c1-86558f94576c",
   "metadata": {},
   "source": [
    "## 2-Layer MLP Forward Pass (with ReLU)\n",
    "\n",
    "**Goal.** Implement the forward pass of a 2-layer multilayer perceptron (MLP) for a mini-batch.\n",
    "\n",
    "**Notation & Shapes**\n",
    "- $X \\in \\mathbb{R}^{N \\times d_{\\text{in}}}$: input batch (N = batch size).\n",
    "- $W_1 \\in \\mathbb{R}^{d_{\\text{in}} \\times h}$, $b_1 \\in \\mathbb{R}^{h}$: first layer params.\n",
    "- $W_2 \\in \\mathbb{R}^{h \\times d_{\\text{out}}}$, $b_2 \\in \\mathbb{R}^{d_{\\text{out}}}$: second layer params.\n",
    "\n",
    "**Forward equations**\n",
    "$$\n",
    "Z_1 = X W_1 + b_1,\\qquad\n",
    "A_1 = \\mathrm{ReLU}(Z_1) = \\max(0, Z_1),\\qquad\n",
    "Z_2 = A_1 W_2 + b_2.\n",
    "$$\n",
    "\n",
    "- For classification, $Z_2$ are logits. Probabilities (optional):\n",
    "$$\n",
    "\\hat{Y} = \\mathrm{softmax}(Z_2).\n",
    "$$\n",
    "\n",
    "**Key ideas**\n",
    "- Bias terms are broadcast across the batch dimension.\n",
    "- ReLU keeps positive values and zeros out negatives.\n",
    "- With ReLU, He/Kaiming initialization keeps variances stable layer-to-layer.\n",
    "\n",
    "**Complexity**\n",
    "- Dominated by matrix multiplies: $O(N\\,d_{\\text{in}}\\,h) + O(N\\,h\\,d_{\\text{out}})$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4021175b-d7f9-44d3-9414-0c2ab8c48ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36130364-b927-4453-b66c-da7f5d5fd3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 shape: torch.Size([8, 32])\n",
      "A1 shape: torch.Size([8, 32])\n",
      "Z2 (logits) shape: torch.Size([8, 10])\n",
      "probs rows sum to ~1: 1.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Hyperparameters\n",
    "N = 8             # batch size\n",
    "d_in = 16         # input features\n",
    "h = 32            # hidden size\n",
    "d_out = 10        # output size\n",
    "\n",
    "# Fake input batch\n",
    "X = torch.randn(N, d_in)\n",
    "\n",
    "# He/Kaiming init for ReLU layers\n",
    "# W1 ~ N(0, sqrt(2/d_in)), W2 ~ N(0, sqrt(2/h))\n",
    "W1 = torch.randn(d_in, h) * (2.0 / d_in) ** 0.5\n",
    "b1 = torch.zeros(h)\n",
    "\n",
    "W2 = torch.randn(h, d_out) * (2.0 / h) ** 0.5\n",
    "b2 = torch.zeros(d_out)\n",
    "\n",
    "# Forward pass\n",
    "# 1) First affine\n",
    "Z1 = X @ W1 + b1 \n",
    "\n",
    "# 2) ReLU\n",
    "A1 = torch.clamp(Z1, min=0.0) \n",
    "\n",
    "# 3) Second affine (logits)\n",
    "Z2 = A1 @ W2 + b2\n",
    "\n",
    "print(\"Z1 shape:\", Z1.shape)\n",
    "print(\"A1 shape:\", A1.shape)\n",
    "print(\"Z2 (logits) shape:\", Z2.shape)\n",
    "\n",
    "probs = torch.softmax(Z2, dim=1)\n",
    "print(\"probs rows sum to ~1:\", probs[0].sum().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33c320c-2633-47f7-a77a-2e30cb852660",
   "metadata": {},
   "source": [
    "### MLP with `nn.Module` / `nn.Sequential`\n",
    "\n",
    "We can build the same two-layer network with high-level building blocks:\n",
    "\n",
    "- `nn.Linear(d_in, h)` → `nn.ReLU()` → `nn.Linear(h, d_out)`\n",
    "\n",
    "By default, `nn.Linear` includes a learnable bias. For ReLU networks, it’s common to use\n",
    "Kaiming/He initialization:\n",
    "```python\n",
    "nn.init.kaiming_normal_(layer.weight, nonlinearity=\"relu\")\n",
    "nn.init.zeros_(layer.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee72b49-ede5-42f3-9741-5796ab804196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: torch.Size([8, 10])\n",
      "probs row sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "d_in, h, d_out = 16, 32, 10\n",
    "\n",
    "# Option A: nn.Sequential\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(d_in, h),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(h, d_out)   # logits\n",
    ")\n",
    "\n",
    "# (Optional) Kaiming init for both Linear layers\n",
    "for m in mlp:\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "X = torch.randn(8, d_in)\n",
    "logits = mlp(X)                 # forward\n",
    "probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "print(\"logits shape:\", logits.shape)\n",
    "print(\"probs row sum:\", probs[0].sum().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af52db0-6d54-4ce8-bd89-a0ffe424d7f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6712b2a7-1ec6-402c-857d-d99f9159c3bb",
   "metadata": {},
   "source": [
    "# Full Version\n",
    "## 2-Layer MLP (ReLU): class, loss/opt, and a full training loop\n",
    "\n",
    "**What these cell (below) does**\n",
    "- Defines a small 2-layer MLP: `Linear(d_in → h) → ReLU → Linear(h → d_out)`  \n",
    "- Builds a *learnable* synthetic multi-class dataset (labels made by a fixed “teacher” MLP)  \n",
    "- Sets `CrossEntropyLoss` (logits + integer labels) and `Adam` optimizer  \n",
    "- Trains with mini-batches, prints train/val loss and accuracy each epoch\n",
    "\n",
    "---\n",
    "\n",
    "### Shapes & forward equations\n",
    "Let a batch $X \\in \\mathbb{R}^{N \\times d_{\\text{in}}}$.\n",
    "\n",
    "- $W_1 \\in \\mathbb{R}^{d_{\\text{in}} \\times h},\\; b_1 \\in \\mathbb{R}^{h}$  \n",
    "- $W_2 \\in \\mathbb{R}^{h \\times d_{\\text{out}}},\\; b_2 \\in \\mathbb{R}^{d_{\\text{out}}}$\n",
    "\n",
    "**Forward pass**\n",
    "$$\n",
    "Z_1 = X W_1 + b_1, \\qquad\n",
    "A_1 = \\mathrm{ReLU}(Z_1) = \\max(0, Z_1), \\qquad\n",
    "Z_2 = A_1 W_2 + b_2 \\quad (\\text{logits})\n",
    "$$\n",
    "\n",
    "For classification, probabilities are:\n",
    "$$\n",
    "p_{i,k} = \\frac{\\exp(Z_{2,i,k})}{\\sum_{j=1}^{d_{\\text{out}}} \\exp(Z_{2,i,j})}\n",
    "$$\n",
    "(Softmax is **implicit** inside `CrossEntropyLoss`, so we pass raw logits.)\n",
    "\n",
    "**Cross-entropy loss (averaged over the batch)**\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{N}\\sum_{i=1}^{N} \\log \\frac{\\exp(Z_{2,i,y_i})}{\\sum_{k=1}^{d_{\\text{out}}} \\exp(Z_{2,i,k})}\n",
    "$$\n",
    "\n",
    "**Accuracy**\n",
    "$$\n",
    "\\text{acc} = \\frac{1}{N}\\sum_{i=1}^{N} \\mathbf{1}\\!\\left[\\arg\\max_k Z_{2,i,k} = y_i\\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Initialization & choices\n",
    "- ReLU networks benefit from **He/Kaiming** initialization. We use:\n",
    "  - `kaiming_normal_` for `fc1.weight` (nonlinearity=\"relu\")  \n",
    "  - zeros for biases  \n",
    "- `CrossEntropyLoss` expects:\n",
    "  - `logits` of shape `(N, d_out)`  \n",
    "  - integer labels of shape `(N,)` in `[0, d_out)`  \n",
    "- Optimizer: `Adam(lr=1e-3)` (tweak `lr`, `weight_decay` as needed)\n",
    "\n",
    "---\n",
    "\n",
    "### Hyperparameters to tweak quickly\n",
    "- `hidden` (width of the hidden layer)  \n",
    "- `epochs`, `batch_size`, learning rate `lr`  \n",
    "- `weight_decay` (L2 regularization)  \n",
    "- Replace the synthetic dataset with your real `TensorDataset`/`DataLoader`\n",
    "\n",
    "---\n",
    "\n",
    "### Sanity checks\n",
    "- Shapes: `Z1, A1 ∈ ℝ^{N×h}`, `logits ∈ ℝ^{N×d_out}`  \n",
    "- `A1` should be **non-negative** (ReLU)  \n",
    "- If you compute `softmax(logits, dim=1)`, each row should sum to ~1\n",
    "\n",
    "---\n",
    "\n",
    "### Extensions (next days)\n",
    "- Add `Dropout` or `BatchNorm1d` between layers  \n",
    "- Try a deeper MLP or different activations (GELU, LeakyReLU)  \n",
    "- Add learning-rate schedules or gradient clipping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dade6388-dfed-4c5b-9b1a-c36c75a2c8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8605eed0-4ccb-4f48-9e67-e941432edc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_in: int, h: int, d_out: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_in, h)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(h, d_out)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # He/Kaiming init for ReLU layers\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity=\"relu\")\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity=\"linear\")\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)   # logits\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68cc76c7-f2e1-4d91-88c7-27d626177980",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3000        # total samples\n",
    "d_in = 16       # input features\n",
    "h_true = 32     # hidden (teacher)\n",
    "num_classes = 5\n",
    "\n",
    "X = torch.randn(N, d_in)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # teacher network (fixed random weights)\n",
    "    W1_t = torch.randn(d_in, h_true) * (2.0/d_in) ** 0.5\n",
    "    b1_t = torch.zeros(h_true)\n",
    "    W2_t = torch.randn(h_true, num_classes) * (2.0/h_true) ** 0.5\n",
    "    b2_t = torch.zeros(num_classes)\n",
    "\n",
    "    Z1_t = X @ W1_t + b1_t\n",
    "    A1_t = torch.clamp(Z1_t, min=0.0)\n",
    "    logits_t = A1_t @ W2_t + b2_t\n",
    "    y = torch.argmax(logits_t, dim=1)   # integer labels in [0, num_classes)\n",
    "\n",
    "# Train/val split\n",
    "train_ratio = 0.8\n",
    "n_train = int(N * train_ratio)\n",
    "n_val = N - n_train\n",
    "dataset = TensorDataset(X, y)\n",
    "train_ds, val_ds = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f310b42-c94e-4253-9b7e-0f750855ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model, loss, optimizer\n",
    "hidden = 64\n",
    "model = MLP(d_in=d_in, h=hidden, d_out=num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1676b15-e30e-483c-bf71-2456ed72e8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss 1.6169 acc 0.327 | val loss 1.3342 acc 0.463\n",
      "Epoch 02 | train loss 1.2023 acc 0.536 | val loss 1.1265 acc 0.579\n",
      "Epoch 03 | train loss 1.0246 acc 0.624 | val loss 1.0110 acc 0.637\n",
      "Epoch 04 | train loss 0.9139 acc 0.676 | val loss 0.9250 acc 0.683\n",
      "Epoch 05 | train loss 0.8335 acc 0.708 | val loss 0.8629 acc 0.706\n",
      "Epoch 06 | train loss 0.7758 acc 0.725 | val loss 0.8144 acc 0.726\n",
      "Epoch 07 | train loss 0.7297 acc 0.741 | val loss 0.7771 acc 0.728\n",
      "Epoch 08 | train loss 0.6918 acc 0.756 | val loss 0.7442 acc 0.735\n",
      "Epoch 09 | train loss 0.6611 acc 0.765 | val loss 0.7171 acc 0.748\n",
      "Epoch 10 | train loss 0.6361 acc 0.777 | val loss 0.6940 acc 0.755\n",
      "Epoch 11 | train loss 0.6145 acc 0.784 | val loss 0.6762 acc 0.763\n",
      "Epoch 12 | train loss 0.5927 acc 0.791 | val loss 0.6578 acc 0.774\n",
      "Epoch 13 | train loss 0.5755 acc 0.794 | val loss 0.6432 acc 0.773\n",
      "Epoch 14 | train loss 0.5605 acc 0.799 | val loss 0.6293 acc 0.781\n",
      "Epoch 15 | train loss 0.5471 acc 0.801 | val loss 0.6165 acc 0.784\n",
      "Epoch 16 | train loss 0.5333 acc 0.809 | val loss 0.6062 acc 0.786\n",
      "Epoch 17 | train loss 0.5205 acc 0.811 | val loss 0.5955 acc 0.792\n",
      "Epoch 18 | train loss 0.5107 acc 0.814 | val loss 0.5858 acc 0.792\n",
      "Epoch 19 | train loss 0.5017 acc 0.815 | val loss 0.5781 acc 0.800\n",
      "Epoch 20 | train loss 0.4907 acc 0.819 | val loss 0.5691 acc 0.798\n",
      "Epoch 21 | train loss 0.4832 acc 0.824 | val loss 0.5612 acc 0.801\n",
      "Epoch 22 | train loss 0.4729 acc 0.825 | val loss 0.5537 acc 0.806\n",
      "Epoch 23 | train loss 0.4643 acc 0.826 | val loss 0.5465 acc 0.808\n",
      "Epoch 24 | train loss 0.4569 acc 0.834 | val loss 0.5406 acc 0.816\n",
      "Epoch 25 | train loss 0.4455 acc 0.838 | val loss 0.5334 acc 0.821\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "def accuracy(logits, targets):\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    return (preds == targets).float().mean().item()\n",
    "\n",
    "epochs = 25\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # ---- train ----\n",
    "    model.train()\n",
    "    running_loss, running_acc, n_batches = 0.0, 0.0, 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_acc  += accuracy(logits.detach(), yb)\n",
    "        n_batches    += 1\n",
    "\n",
    "    train_loss = running_loss / n_batches\n",
    "    train_acc  = running_acc  / n_batches\n",
    "\n",
    "    # ---- validate ----\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_acc, n_batches = 0.0, 0.0, 0\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            val_loss += loss.item()\n",
    "            val_acc  += accuracy(logits, yb)\n",
    "            n_batches += 1\n",
    "\n",
    "    val_loss /= n_batches\n",
    "    val_acc  /= n_batches\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"train loss {train_loss:.4f} acc {train_acc:.3f} | \"\n",
    "          f\"val loss {val_loss:.4f} acc {val_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917b60a4-1e3a-4e4e-b492-f337468d502e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-dev)",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
