{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e7f92c-e02b-45a6-b969-ebe2b9ff9345",
   "metadata": {},
   "source": [
    "# 2-Layer MLP Backpropagation (ReLU + Softmax-CE)\n",
    "\n",
    "We implement a 2-layer Multi-Layer Perceptron (MLP) and its **backward pass (backpropagation)**.\n",
    "\n",
    "---\n",
    "\n",
    "### Forward pass\n",
    "\n",
    "- Input: $ X \\in \\mathbb{R}^{N \\times d} $ \n",
    "- First affine:  \n",
    "  $$\n",
    "  Z_1 = X W_1 + b_1, \\quad W_1 \\in \\mathbb{R}^{d \\times H}, \\, b_1 \\in \\mathbb{R}^H\n",
    "  $$\n",
    "- ReLU activation:  \n",
    "  $$\n",
    "  H_1 = \\max(0, Z_1)\n",
    "  $$\n",
    "- Second affine (logits):  \n",
    "  $$\n",
    "  S = H_1 W_2 + b_2, \\quad W_2 \\in \\mathbb{R}^{H \\times K}, \\, b_2 \\in \\mathbb{R}^K\n",
    "  $$\n",
    "- Softmax probabilities:  \n",
    "  $$\n",
    "  P = \\mathrm{softmax}(S)\n",
    "  $$\n",
    "- Cross-entropy loss (with one-hot labels $Y$):  \n",
    "  $$\n",
    "  \\mathcal{L} = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{k=1}^{K} Y_{ik}\\,\\log P_{ik}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### Backward pass\n",
    "\n",
    "Define gradient at logits:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial S} = \\frac{P - Y}{N}.\n",
    "$$\n",
    "\n",
    "Then:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_2} &= H_1^\\top \\frac{\\partial \\mathcal{L}}{\\partial S}, \\quad &\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_2} &= \\mathbf{1}^\\top \\frac{\\partial \\mathcal{L}}{\\partial S}, \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial H_1} &= \\left(\\frac{\\partial \\mathcal{L}}{\\partial S}\\right) W_2^\\top, \\quad &\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial Z_1} &= \\frac{\\partial \\mathcal{L}}{\\partial H_1} \\odot \\mathbf{1}[Z_1 > 0], \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_1} &= X^\\top \\frac{\\partial \\mathcal{L}}{\\partial Z_1}, \\quad &\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_1} &= \\mathbf{1}^\\top \\frac{\\partial \\mathcal{L}}{\\partial Z_1}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Parameter update (SGD)\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\, \\frac{\\partial \\mathcal{L}}{\\partial \\theta}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "This forms the full **manual backpropagation** for a 2-layer MLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0383bd-8351-4d8a-8bb7-a0a9a8a1fe0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f879c2cc-d039-4739-a208-9499469f0709",
   "metadata": {},
   "source": [
    "### ðŸ”§ Manual Backprop for a 2-Layer MLP\n",
    "\n",
    "This block implements a **2-layer MLP with ReLU + Softmax-CrossEntropy** using **manual backpropagation** (no autograd).  \n",
    "\n",
    "Steps:\n",
    "1. **Forward pass**  \n",
    "   - Compute hidden layer: $Z_1 = XW_1 + b_1$, apply ReLU.  \n",
    "   - Compute logits: $S = H_1 W_2 + b_2$.  \n",
    "   - Apply softmax for class probabilities.  \n",
    "   - Compute cross-entropy loss.  \n",
    "\n",
    "2. **Backward pass**  \n",
    "   - Start from $\\frac{\\partial \\mathcal{L}}{\\partial S} = (P-Y)/N$.  \n",
    "   - Propagate gradients backward through $W_2, b_2$, then through ReLU, then $W_1, b_1$.  \n",
    "\n",
    "3. **Update step**  \n",
    "   - Parameters updated with vanilla SGD:  \n",
    "     $$\n",
    "     \\theta \\leftarrow \\theta - \\eta \\, \\frac{\\partial \\mathcal{L}}{\\partial \\theta}.\n",
    "     $$\n",
    "\n",
    "This shows the math of backprop in code, without relying on PyTorchâ€™s `autograd`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d0798c7-259f-4172-81a5-6b6b8cdb432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  50 | loss 0.9842 | acc 0.625\n",
      "epoch 100 | loss 0.3256 | acc 0.969\n",
      "epoch 150 | loss 0.1020 | acc 1.000\n",
      "epoch 200 | loss 0.0477 | acc 1.000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Tiny toy dataset (N,d) -> K classes\n",
    "N, d, H, K = 128, 20, 64, 5\n",
    "X = torch.randn(N, d)\n",
    "y = torch.randint(0, K, (N,))\n",
    "\n",
    "# One-hot labels\n",
    "Y = torch.zeros(N, K)\n",
    "Y[torch.arange(N), y] = 1.0\n",
    "\n",
    "# Parameters\n",
    "W1 = torch.randn(d, H) * 0.02\n",
    "b1 = torch.zeros(H)\n",
    "W2 = torch.randn(H, K) * 0.02\n",
    "b2 = torch.zeros(K)\n",
    "\n",
    "lr = 0.5\n",
    "epochs = 200\n",
    "\n",
    "def softmax(logits):\n",
    "    # stable softmax\n",
    "    z = logits - logits.max(dim=1, keepdim=True).values\n",
    "    expz = torch.exp(z)\n",
    "    return expz / expz.sum(dim=1, keepdim=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward\n",
    "    Z1 = X @ W1 + b1\n",
    "    H1 = torch.clamp(Z1, min=0.0)\n",
    "    S  = H1 @ W2 + b2\n",
    "    P  = softmax(S)\n",
    "\n",
    "    # Cross-entropy\n",
    "    loss = -(Y * (P+1e-12).log()).sum() / N\n",
    "\n",
    "    # Backward (manual)\n",
    "    dS  = (P - Y) / N\n",
    "    dW2 = H1.t() @ dS\n",
    "    db2 = dS.sum(dim=0)\n",
    "\n",
    "    dH1 = dS @ W2.t()\n",
    "    dZ1 = dH1 * (Z1 > 0).float()\n",
    "\n",
    "    dW1 = X.t() @ dZ1\n",
    "    db1 = dZ1.sum(dim=0)\n",
    "\n",
    "    # SGD step\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        pred = P.argmax(dim=1)\n",
    "        acc = (pred == y).float().mean().item()\n",
    "        print(f\"epoch {epoch+1:3d} | loss {loss.item():.4f} | acc {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6374c121-997e-459c-ab57-22d02f282c37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b987669f-d3e7-4966-aad7-e6e60f834b5f",
   "metadata": {},
   "source": [
    "### âš¡ 2-Layer MLP with Autograd (`nn.Sequential`)\n",
    "\n",
    "In this version, we let **PyTorchâ€™s autograd** handle the backward pass.  \n",
    "\n",
    "- The model is defined with `nn.Sequential`:  \n",
    "  1. Linear layer $(d \\to H$)  \n",
    "  2. ReLU activation  \n",
    "  3. Linear layer ($H \\to K$)  \n",
    "\n",
    "- Loss: `nn.CrossEntropyLoss`, which internally applies **log-softmax + NLL**.  \n",
    "- Optimizer: `torch.optim.SGD` updates parameters automatically after `loss.backward()`.\n",
    "\n",
    "This shows the same network as before, but with **automatic differentiation** instead of manual gradient formulas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18b749de-0455-47fc-a077-795f7bccd6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  50 | loss 0.4812 | acc 0.922\n",
      "epoch 100 | loss 0.1280 | acc 1.000\n",
      "epoch 150 | loss 0.0550 | acc 1.000\n",
      "epoch 200 | loss 0.0318 | acc 1.000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Same synthetic data\n",
    "N, d, H, K = 128, 20, 64, 5\n",
    "X = torch.randn(N, d)\n",
    "y = torch.randint(0, K, (N,))\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(d, H),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H, K)\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "for epoch in range(200):\n",
    "    optim.zero_grad()\n",
    "    logits = model(X)\n",
    "    loss = criterion(logits, y)\n",
    "    loss.backward()                        # autograd handles backprop\n",
    "    optim.step()\n",
    "\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        acc = (logits.argmax(1) == y).float().mean().item()\n",
    "        print(f\"epoch {epoch+1:3d} | loss {loss.item():.4f} | acc {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef18ce1-a8b3-427b-9608-76354f4c89c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-dev)",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
