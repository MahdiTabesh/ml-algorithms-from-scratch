{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c5b96a5-3ba7-4256-b8b3-1475897d6837",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "**Goal:** Normalize activations within a mini-batch to stabilize and speed up training.\n",
    "\n",
    "### Training Mode\n",
    "\n",
    "For a batch of inputs $x = (x_1, \\dots, x_m)$:\n",
    "\n",
    "1. **Batch mean**  \n",
    "   $$ \\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i $$\n",
    "\n",
    "2. **Batch variance**  \n",
    "   $$ \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2 $$\n",
    "\n",
    "3. **Normalize**  \n",
    "   $$ \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} $$\n",
    "\n",
    "4. **Scale and shift (learned parameters)**  \n",
    "   $$ y_i = \\gamma \\hat{x}_i + \\beta $$\n",
    "\n",
    "- $\\gamma$: scaling factor (learned)  \n",
    "- $\\beta$: shift factor (learned)  \n",
    "- $\\epsilon$: small constant for numerical stability  \n",
    "\n",
    "### Inference Mode\n",
    "- Use running averages of $\\mu$ and $\\sigma^2$ instead of batch statistics:\n",
    "  $$ y_i = \\gamma \\cdot \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta $$\n",
    "\n",
    "### Benefits\n",
    "- Reduces *internal covariate shift*  \n",
    "- Enables higher learning rates  \n",
    "- Adds regularization effect  \n",
    "- Improves convergence speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccd89c5-e761-4ff8-908a-d4b5bc3a42a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ad09324-1fd9-4479-b48f-5bc69369125f",
   "metadata": {},
   "source": [
    "## Batch Normalization Forward (NumPy Implementation)\n",
    "\n",
    "We implement BatchNorm manually for both **training** and **inference**.\n",
    "\n",
    "- During **training**, batch statistics ($\\mu_B, \\sigma_B^2$) are used.  \n",
    "- During **inference**, running averages of mean/variance are used instead.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b5c37b-d22b-4846-95c1-3a1d4e4895f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ac36e6-fbdf-4bd6-a849-a0ec96fa673e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training forward:\n",
      "[[ 0.59662445 -0.2123117   0.67702389]\n",
      " [ 1.2697554   1.67649844 -1.39017913]\n",
      " [-0.55240527 -0.92221048 -0.46643536]\n",
      " [-1.31397458 -0.54197626  1.17959061]]\n",
      "\n",
      "Inference forward:\n",
      "[[ 7.27530483  1.39868512  3.15781239]\n",
      " [ 9.40374094  7.37118196 -3.37892902]\n",
      " [ 3.64207961 -0.84604458 -0.45794154]\n",
      " [ 1.23400192  0.35627195  4.74698802]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BatchNorm:\n",
    "    def __init__(self, dim, momentum=0.9, eps=1e-5):\n",
    "        self.gamma = np.ones((1, dim))   # scale\n",
    "        self.beta = np.zeros((1, dim))   # shift\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "        # running stats (for inference)\n",
    "        self.running_mean = np.zeros((1, dim))\n",
    "        self.running_var = np.zeros((1, dim))\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        if training:\n",
    "            batch_mean = np.mean(x, axis=0, keepdims=True)\n",
    "            batch_var = np.var(x, axis=0, keepdims=True)\n",
    "\n",
    "            # normalize\n",
    "            x_hat = (x - batch_mean) / np.sqrt(batch_var + self.eps)\n",
    "            out = self.gamma * x_hat + self.beta\n",
    "\n",
    "            # update running stats\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var\n",
    "\n",
    "            return out\n",
    "        else:\n",
    "            # inference: use running averages\n",
    "            x_hat = (x - self.running_mean) / np.sqrt(self.running_var + self.eps)\n",
    "            out = self.gamma * x_hat + self.beta\n",
    "            return out\n",
    "\n",
    "\n",
    "# Example usage\n",
    "np.random.seed(0)\n",
    "x = np.random.randn(4, 3)  # batch of 4, 3 features\n",
    "bn = BatchNorm(dim=3)\n",
    "\n",
    "print(\"Training forward:\")\n",
    "print(bn.forward(x, training=True))\n",
    "\n",
    "print(\"\\nInference forward:\")\n",
    "print(bn.forward(x, training=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d9d2b2-d434-48b6-98dd-f5fc2f319477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31e16f91-2bba-45e7-9add-f93a277421f5",
   "metadata": {},
   "source": [
    "## Batch Normalization Forward (PyTorch)\n",
    "\n",
    "PyTorch provides `nn.BatchNorm1d`, `nn.BatchNorm2d`, and `nn.BatchNorm3d` for \n",
    "different input shapes.\n",
    "\n",
    "- `BatchNorm1d`: for fully-connected layers (2D input: `[batch, features]`)\n",
    "- `BatchNorm2d`: for CNNs with images (4D input: `[batch, channels, H, W]`)\n",
    "\n",
    "The layer automatically:\n",
    "1. Computes batch mean and variance during training.  \n",
    "2. Uses running averages during inference.  \n",
    "3. Maintains learnable parameters $\\gamma$ and $\\beta$.\n",
    "\n",
    "Formula:\n",
    "$$ y_i = \\gamma \\cdot \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "784d140a-6586-4080-b5f9-ad047da8efea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training forward:\n",
      " tensor([[-9.7447e-01,  2.0465e-05,  1.9892e-01],\n",
      "        [-8.9459e-01, -1.4601e+00, -1.2333e+00],\n",
      "        [ 1.4361e+00,  1.3634e+00, -4.5864e-01],\n",
      "        [ 4.3300e-01,  9.6719e-02,  1.4930e+00]],\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "\n",
      "Inference forward:\n",
      " tensor([[ 0.3533, -0.2773,  0.3221],\n",
      "        [ 0.3810, -1.9022, -0.4627],\n",
      "        [ 1.1895,  1.2399, -0.0382],\n",
      "        [ 0.8415, -0.1697,  1.0313]], grad_fn=<NativeBatchNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example: BatchNorm for fully connected layer\n",
    "bn = nn.BatchNorm1d(num_features=3)\n",
    "\n",
    "# Fake input: batch of 4 samples, 3 features each\n",
    "x = torch.randn(4, 3)\n",
    "\n",
    "# Training mode\n",
    "bn.train()\n",
    "y_train = bn(x)\n",
    "print(\"Training forward:\\n\", y_train)\n",
    "\n",
    "# Inference mode\n",
    "bn.eval()\n",
    "y_test = bn(x)\n",
    "print(\"\\nInference forward:\\n\", y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982aed97-fdff-4427-8ca3-adb0967c344c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef4a783e-3776-4eb9-8e8e-584ece8356d9",
   "metadata": {},
   "source": [
    "## BatchNorm in a CNN (Conv2d → BN → ReLU)\n",
    "\n",
    "**Why BN here?**  \n",
    "In CNNs, BatchNorm is applied **per channel** after a convolution and before the nonlinearity:\n",
    "\n",
    "> `Conv2d → BatchNorm2d → ReLU → (optional MaxPool / Dropout)`\n",
    "\n",
    "For a 4D input $x \\in \\mathbb{R}^{N \\times C \\times H \\times W}$, `BatchNorm2d(C)` normalizes each channel using the batch’s spatial activations:\n",
    "\n",
    "- **Training stats** (per channel):  \n",
    "  $$\\mu_c = \\frac{1}{N\\!HW}\\sum_{n,h,w} x_{n,c,h,w}, \\quad\n",
    "    \\sigma_c^2 = \\frac{1}{N\\!HW}\\sum_{n,h,w} (x_{n,c,h,w}-\\mu_c)^2$$\n",
    "- **Normalize, then scale/shift**:  \n",
    "  $$\\hat{x}_{n,c,h,w}=\\frac{x_{n,c,h,w}-\\mu_c}{\\sqrt{\\sigma_c^2+\\epsilon}}, \\quad\n",
    "    y_{n,c,h,w}=\\gamma_c \\hat{x}_{n,c,h,w}+\\beta_c$$\n",
    "\n",
    "During **inference**, running means/vars accumulated during training are used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48b1818e-b917-46b7-ae72-6540f5ef51e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmallBNConvNet(\n",
      "  (block1): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=2048, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "[TRAIN] logits shape: torch.Size([8, 10]), loss: 2.6602\n",
      "[EVAL ] logits shape: torch.Size([8, 10])\n",
      "\n",
      "BN1 running_mean shape: torch.Size([16])\n",
      "BN1 running_var   shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch CNN with BatchNorm2d — full working demo\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a tiny CNN\n",
    "class SmallBNConvNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 8 * 8, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "# Create dummy image batch\n",
    "torch.manual_seed(0)\n",
    "N, C, H, W = 8, 3, 32, 32\n",
    "X = torch.randn(N, C, H, W)\n",
    "y = torch.randint(0, 10, (N,)) # dummy labels for 10 classes\n",
    "\n",
    "# Instantiate model, loss, optimizer \n",
    "model = SmallBNConvNet(in_channels=C, num_classes=10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# TRAINING mode\n",
    "model.train()\n",
    "logits_train = model(X)\n",
    "loss = criterion(logits_train, y)\n",
    "opt.zero_grad()\n",
    "loss.backward()\n",
    "opt.step()\n",
    "\n",
    "print(f\"\\n[TRAIN] logits shape: {logits_train.shape}, loss: {loss.item():.4f}\")\n",
    "\n",
    "# EVAL/INFERENCE mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits_eval = model(X)  # same X just to show it runs; normally we'd use new data\n",
    "print(f\"[EVAL ] logits shape: {logits_eval.shape}\")\n",
    "\n",
    "# check a BN layer's running stats\n",
    "bn1 = model.block1[1]\n",
    "print(f\"\\nBN1 running_mean shape: {bn1.running_mean.shape}\")\n",
    "print(f\"BN1 running_var   shape: {bn1.running_var.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bffd18-783a-4ecf-9fea-318393a026f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-dev)",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
